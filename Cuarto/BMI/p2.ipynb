{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Eq_QfGIGXC_"
   },
   "source": [
    "### **Búsqueda y Minería de Información 2022-23**\n",
    "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
    "### Grado en Ingeniería Informática, 4º curso\n",
    "# **Motores de búsqueda e indexación**\n",
    "\n",
    "Fechas:\n",
    "\n",
    "* Comienzo: martes 21 / jueves 23 de febrero\n",
    "* Entrega: martes 28 / jueves 30 de marzo (14:00)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores\n",
    "\n",
    "Guillermo Martín-Coello & Daniel Varela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYT0Qlrnoy7l"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDFY_K6_pA_J"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Los objetivos de esta práctica son:\n",
    "\n",
    "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
    "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
    "*\tLa implementación de un método de búsqueda proximal.\n",
    "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
    "*\tLa implementación del algoritmo PageRank.\n",
    "\n",
    "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
    "\n",
    "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
    "\n",
    "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El **único buscador que no será intercambiable es el de Whoosh**, que sólo funcionará con sus propios índices."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calificación\n",
    "\n",
    "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
    "\n",
    "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
    "\n",
    "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
    "\n",
    "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
    "\n",
    "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la *celda de prueba* así como otros tests adicionales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrega\n",
    "\n",
    "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p2-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicaciones\n",
    "\n",
    "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
    "\n",
    "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la **celda de prueba** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (ten en cuenta que, aquellos ejercicios que no se hayan realizado, lanzan una excepción que se captura en dicha celda, por lo que no debería ser necesario modificarla).\n",
    "\n",
    "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPjq_DVVpDEL"
   },
   "source": [
    "## Material proporcionado\n",
    "\n",
    "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
    "\n",
    "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
    "Las clases parten del código de la práctica anterior.\n",
    "Igual que en la práctica 1, la **celda de prueba** (al final del enunciado) implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
    "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
    "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
    "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
    "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería mantenerse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases genéricas ya implementadas\n",
    "\n",
    "En la siguiente celda de código, se encuentran ya implementadas las clases *Index* y *Builder* de manera que facilite la creación de otros índices a partir de las mismas. \n",
    "\n",
    "Estudia esta implementación y compara las **decisiones de diseño** tomadas con las vuestras en la práctica anterior.\n",
    "Ten en cuenta que las funciones de TF e IDF están **sin implementar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "xAKBQZLLqVXR"
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import zipfile\n",
    "from abc import ABC, abstractmethod\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "class Config(object):\n",
    "  # variables de clase\n",
    "  NORMS_FILE = \"docnorms.dat\"\n",
    "  PATHS_FILE = \"docpaths.dat\"\n",
    "  INDEX_FILE = \"serialindex.dat\"\n",
    "  DICTIONARY_FILE = \"dictionary.dat\"\n",
    "  POSTINGS_FILE = \"postings.dat\"\n",
    "  POSITIONS_FILE = \"positions.dat\"\n",
    "\n",
    "\n",
    "class BasicParser:\n",
    "    @staticmethod\n",
    "    def parse(text):\n",
    "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
    "\n",
    "def tf(freq):\n",
    "    \"\"\"\n",
    "    tf(freq)\n",
    "        - Return the term frequency.\n",
    "    @param freq: term frequency\n",
    "    \"\"\"\n",
    "    return 1 + math.log2(freq) if freq > 0 else 0\n",
    "\n",
    "\n",
    "def idf(df, n):\n",
    "    \"\"\"\n",
    "    idf(df, n)\n",
    "        - Return the inverse document frequency.\n",
    "    @param df: document frequency\n",
    "    @param n: number of documents\n",
    "    \"\"\"\n",
    "    return math.log2((n + 1) / (df + 0.5))\n",
    "\n",
    "\"\"\"\n",
    "    This is an abstract class for the search engines\n",
    "\"\"\"\n",
    "class Searcher(ABC):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        self.index = index\n",
    "        self.parser = parser\n",
    "    @abstractmethod\n",
    "    def search(self, query, cutoff):\n",
    "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
    "\n",
    "class Index:\n",
    "    def __init__(self, dir=None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        if dir: self.open(dir)\n",
    "    def add_doc(self, path):\n",
    "        self.docmap.append(path)\n",
    "    def doc_path(self, docid):\n",
    "        return self.docmap[docid]\n",
    "    def doc_module(self, docid):\n",
    "        if docid in self.modulemap:\n",
    "            return self.modulemap[docid]\n",
    "        return None\n",
    "    def ndocs(self):\n",
    "        return len(self.docmap)\n",
    "    def doc_freq(self, term):\n",
    "        return len(self.postings(term))\n",
    "    def term_freq(self, term, docID):\n",
    "        post = self.postings(term)\n",
    "        if post is None: return 0\n",
    "        for posting in post:\n",
    "            if posting[0] == docID:\n",
    "                return posting[1]\n",
    "        return 0\n",
    "    def total_freq(self, term):\n",
    "        freq = 0\n",
    "        for posting in self.postings(term):\n",
    "            freq += posting[1]\n",
    "        return freq\n",
    "    def postings(self, term):\n",
    "        # used in more efficient implementations\n",
    "        return list()\n",
    "    def positional_postings(self, term):\n",
    "        # used in positional implementations\n",
    "        return list()\n",
    "    def all_terms(self):\n",
    "        return list()\n",
    "    def save(self, dir):\n",
    "        if not self.modulemap: self.compute_modules()\n",
    "        p = os.path.join(dir, Config.NORMS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.modulemap, f)        \n",
    "    def open(self, dir):\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.NORMS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.modulemap = pickle.load(f)\n",
    "        except OSError:\n",
    "            # the file may not exist the first time\n",
    "            pass\n",
    "    def compute_modules(self):\n",
    "        for term in self.all_terms():\n",
    "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
    "            post = self.postings(term)\n",
    "            if post is None: continue\n",
    "            for docid, freq in post:\n",
    "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
    "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
    "        for docid in range(self.ndocs()):\n",
    "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
    "\n",
    "import shutil\n",
    "class Builder:\n",
    "    def __init__(self, dir, parser=BasicParser()):\n",
    "        if os.path.exists(dir): shutil.rmtree(dir)\n",
    "        os.makedirs(dir)\n",
    "        self.parser = parser\n",
    "    def build(self, path):\n",
    "        if zipfile.is_zipfile(path):\n",
    "            self.index_zip(path)\n",
    "        elif os.path.isdir(path):\n",
    "            self.index_dir(path)\n",
    "        else:\n",
    "            self.index_url_file(path)\n",
    "    def index_zip(self, filename):\n",
    "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
    "        for name in sorted(file.namelist()):\n",
    "            with file.open(name, \"r\", force_zip64=True) as f:\n",
    "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "        file.close()\n",
    "    def index_dir(self, dir):\n",
    "        for subdir, dirs, files in os.walk(dir):\n",
    "            for file in sorted(files):\n",
    "                path = os.path.join(dir, file)\n",
    "                with open(path, \"r\") as f:\n",
    "                    self.index_document(path, f.read())\n",
    "    def index_url_file(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            self.index_urls(line.rstrip('\\n') for line in f)\n",
    "    def index_urls(self, urls):\n",
    "        ctx = ssl.create_default_context()\n",
    "        ctx.check_hostname = False\n",
    "        ctx.verify_mode = ssl.CERT_NONE\n",
    "        for url in urls:\n",
    "            self.index_document(url, BeautifulSoup(urlopen(url, context=ctx).read().decode(\"utf-8\"), \"html.parser\").text)\n",
    "    def index_document(self, path, text):\n",
    "        raise NotImplementedError # to be implemented by child class\n",
    "    def commit(self):\n",
    "        raise NotImplementedError # to be implemented by child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de buscador\n",
    "\n",
    "En la siguiente celda se encuentra una implementación de un buscador basado en coseno que es relativamente lento. En los siguientes ejercicios veremos formas de acelerar el proceso (sin cambiar los resultados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "yMoae4N7y38C"
   },
   "outputs": [],
   "source": [
    "# from previous lab\n",
    "class SlowVSMSearcher(Searcher):\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for docid in range(self.index.ndocs()):\n",
    "            score = self.score(docid, qterms)\n",
    "            if score:\n",
    "                ranking.push(self.index.doc_path(docid), score)\n",
    "        return ranking\n",
    "\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases Whoosh\n",
    "\n",
    "En la siguiente celda podrás encontrar la adaptación a nuestras interfaces de los índices de Whoosh, en concreto, de tres variantes que permite usar la librería (observa los distintos Schema's usados y qué metodos se han reimplementado en cada caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "I-7gj9Rxx6LD"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import whoosh\n",
    "except ModuleNotFoundError:\n",
    "  !pip install whoosh\n",
    "  import whoosh\n",
    "from whoosh.fields import Schema, TEXT, ID\n",
    "from whoosh.formats import Format\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# A schema in Whoosh is the set of possible fields in a document in the search space. \n",
    "# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n",
    "# and a content.\n",
    "SimpleDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False))\n",
    "ForwardDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=False,vector=Format))\n",
    "PositionalDocument = Schema(\n",
    "        path=ID(stored=True),\n",
    "        content=TEXT(phrase=True))\n",
    "\n",
    "class WhooshBuilder(Builder):\n",
    "    def __init__(self, dir, schema=SimpleDocument):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
    "        self.dir = dir\n",
    "\n",
    "    def index_document(self, p, text):\n",
    "        self.whoosh_writer.add_document(path=p, content=text)\n",
    "\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshForwardBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, ForwardDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshForwardIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshPositionalBuilder(WhooshBuilder):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir, PositionalDocument)\n",
    "    def commit(self):\n",
    "        self.whoosh_writer.commit()\n",
    "        index = WhooshPositionalIndex(self.dir)\n",
    "        index.save(self.dir)\n",
    "\n",
    "class WhooshIndex(Index):\n",
    "    def __init__(self, dir):\n",
    "        super().__init__(dir)\n",
    "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
    "    def total_freq(self, term):\n",
    "        return self.whoosh_reader.frequency(\"content\", term)\n",
    "    def doc_freq(self, term):\n",
    "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_reader.stored_fields(docid)['path']\n",
    "    def ndocs(self):\n",
    "        return self.whoosh_reader.doc_count()\n",
    "    def all_terms(self):\n",
    "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
    "    def postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshForwardIndex(WhooshIndex):\n",
    "    def term_freq(self, term, docID) -> int:\n",
    "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
    "            v = self.whoosh_reader.vector(docID, \"content\")\n",
    "            v.skip_to(term)\n",
    "            if v.id() == term:\n",
    "                return v.value_as(\"frequency\")\n",
    "        return 0\n",
    "\n",
    "class WhooshPositionalIndex(WhooshIndex):\n",
    "    def positional_postings(self, term):\n",
    "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
    "            if self.doc_freq(term) > 0 else []\n",
    "\n",
    "class WhooshSearcher(Searcher):\n",
    "    def __init__(self, dir):\n",
    "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
    "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
    "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
    "    def search(self, query, cutoff):\n",
    "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
    "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
    "    def doc_path(self, docid):\n",
    "        return self.whoosh_index.reader().stored_fields(docid)['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3jRLNZmpEk_"
   },
   "source": [
    "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
    "\n",
    "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
    "\n",
    "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be3vDQNxdWbo"
   },
   "source": [
    "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
    "\n",
    "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "ppr9PtZmduql"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create a Term-Based Vector Space Model Searcher\n",
    "\"\"\"\n",
    "class TermBasedVSMSearcher(Searcher):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - index: the index for the search\n",
    "        - parser: the parser for the search\n",
    "    \"\"\"\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "\n",
    "    \"\"\"\n",
    "    Search method to find a query in the index\n",
    "    Parameters:\n",
    "        - query: the query to be searched\n",
    "        - cutoff: number of results to be returned (maximum)\n",
    "    Return:\n",
    "        - SearchRanking with results\n",
    "    \"\"\"\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        postings = {}\n",
    "        for term in qterms:\n",
    "            for post in self.index.postings(term):\n",
    "                if post[0] not in postings.keys():\n",
    "                    postings[post[0]]=0\n",
    "                postings[post[0]] += self.score(post[0], [term])\n",
    "        for doc_id in postings.keys():\n",
    "            if doc_id in postings.keys():\n",
    "                ranking.push(self.index.doc_path(doc_id), postings[doc_id])\n",
    "\n",
    "        return ranking\n",
    "\n",
    "    \"\"\"\n",
    "    Score method to compute the score of a query in a document\n",
    "    Parameters:\n",
    "        - docid: the document id\n",
    "        - qterms: the query terms on a list\n",
    "    Return:\n",
    "        - the score of the query in the document\n",
    "    \"\"\"\n",
    "    def score(self, docid, qterms):\n",
    "        prod = 0\n",
    "        for term in qterms:\n",
    "            prod += tf(self.index.term_freq(term, docid)) \\\n",
    "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "        mod = self.index.doc_module(docid)\n",
    "        if mod:\n",
    "            return prod / mod\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3YGEGm7haop"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para implementar el modelo vectorial con coseno por el método orientado a términos hemos trabajado a partir de la clase abstracta Searcher de la que hemos heredado. Los dos cambios necesarios han sido los siguientes:\n",
    "\n",
    "Para el método search hemos tenido que reescribir su funcionamiento para que utilize el algoritmo del coseno. Éste simplemente obtendrá como argumento los elementos de la query, sobre los que iterará, y para cada uno comprobará los postings guardados en el índice e irá encontrando el score correspondiente de cada documento gracias al método \"score\". los scores obtenidos los añadirá en un ranking para obtener los n primeros, siendo n el cutoff entrado como argumento.\n",
    "\n",
    "Para poder implementar la funcionalidad del método anterior también necesitamos reescribir el método score, que utilizará el algorimo del coseno para calcular el valor de cada palabra en función de su importancia en el documento (tf) y de su valor discriminatorio respecto a todos los documentos (idf). Así, se obtiene el score que utiliza el método search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3ti8qGedgNB"
   },
   "source": [
    "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
    "\n",
    "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "wzZ-6OG0dvwX"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\"\"\"\n",
    "Class to create a Document-Based Vector Space Model Searcher\n",
    "\"\"\"\n",
    "class DocBasedVSMSearcher(Searcher):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - index: the index for the search\n",
    "        - parser: the parser for the search\n",
    "    \"\"\"\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    Search method to find a query in the index\n",
    "    Parameters:\n",
    "        - query: the query to be searched\n",
    "        - cutoff: number of results to be returned (maximum)\n",
    "    Return:\n",
    "        - SearchRanking with results\n",
    "    \"\"\"\n",
    "    def search(self, query, cutoff):\n",
    "        qterms = self.parser.parse(query)\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        postings = []\n",
    "\n",
    "        # Extract all postings from the query terms: postings = (docid, score)\n",
    "        for term in qterms:\n",
    "            for post in self.index.postings(term):\n",
    "                postings.append((post[0], self.score(post[0], post[1], term)))\n",
    "        \n",
    "        # Create new  min heap for postings\n",
    "        heap = postings\n",
    "        heapq.heapify(heap)\n",
    "        if len(heap) <= 0:\n",
    "            print(\"Error, not enough elements in heap\")\n",
    "            return ranking\n",
    "        prev, score = heapq.heappop(heap)\n",
    "        heapq.heapify(heap)\n",
    "        while len(heap) > 0:\n",
    "            curr, aux = heapq.heappop(heap)\n",
    "            if prev == curr:\n",
    "                score += aux\n",
    "            else:\n",
    "                mod = self.index.doc_module(prev)\n",
    "                if mod != 0:\n",
    "                    ranking.push(self.index.doc_path(prev), score/mod)\n",
    "                else:\n",
    "                    \n",
    "                    ranking.push(self.index.doc_path(prev), 0)\n",
    "                prev = curr\n",
    "                score = aux\n",
    "                \n",
    "            heapq.heapify(heap)\n",
    "        mod = self.index.doc_module(curr)\n",
    "        if mod != 0:\n",
    "            ranking.push(self.index.doc_path(curr), score/mod)\n",
    "        else:\n",
    "            \n",
    "            ranking.push(self.index.doc_path(curr), 0)\n",
    "\n",
    "        return ranking\n",
    "\n",
    "    \"\"\"\n",
    "    Score method to compute the score of a query in a document\n",
    "    Parameters:\n",
    "        - docid: the document id\n",
    "        - qterms: the query terms on a list\n",
    "    Return:\n",
    "        - the score of the query in the document\n",
    "    \"\"\"\n",
    "    def score(self, docid, frec, term):\n",
    "        prod = tf(frec) \\\n",
    "                * idf(self.index.doc_freq(term), self.index.ndocs())\n",
    "\n",
    "        return prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7xYd4hzhukr"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para implementar el método orientado a documentos hemos seguido el razonamiento del algoritmo utilizando un heap de postings. Esto consiste en que para cada término de la query extraemos los postings y utilizamos un heap para sacarlos en el orden correcto. Para simplificar el código y evitar repetir llamadas hemos calculado el score de cada elemento directamente en su posting, en vez de guardar la frecuencia y calcularlo más adelante. Cuando se extrae del heap un posting, se comprueba su documento, y, si es diferente al anterior se comienza a guardar el score del nuevo documento, mientras que si es el mismo documento, se suma el score al previamente obtenido en anteriores iteraciones. Mantenemos un último push fuera del bucle para que se añada el último documento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpXHr18Cdl2Q"
   },
   "source": [
    "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
    "\n",
    "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)), es decir, que permita almacenar un **número limitado de documentos** en memoria y su puntuación asociada. \n",
    "\n",
    "Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "MOfT2yZGpMNi"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "\"\"\"\n",
    "Class to create a Search Ranking\n",
    "\"\"\"\n",
    "class SearchRanking:\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - cutoff: number of results to be returned (maximum)\n",
    "    \"\"\"\n",
    "    def __init__(self, cutoff):\n",
    "        self.cutoff = cutoff\n",
    "        self.ranking = [] \n",
    "\n",
    "    \"\"\"\n",
    "    Push method to add a new result to the ranking\n",
    "    Parameters:\n",
    "        - docid: the document id\n",
    "        - score: the score of the document\n",
    "    \"\"\"\n",
    "    def push(self, docid, score):\n",
    "        if (len(self.ranking)<self.cutoff):\n",
    "            heapq.heappush(self.ranking, (docid, score))\n",
    "        else:\n",
    "            heapq.heappushpop(self.ranking, (docid, score))\n",
    "    \n",
    "    \"\"\"\n",
    "    Iterator method to iterate over the ranking\n",
    "    \"\"\"\n",
    "    def __iter__(self):\n",
    "        min_l = min(len(self.ranking), self.cutoff)\n",
    "        ## sort ranking\n",
    "        self.ranking.sort(key=lambda tup: tup[1], reverse=True)\n",
    "        return iter(self.ranking[0:min_l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJDzjUp-hwNZ"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para implementar la clase Search Ranking hemos utilizado la librería heapq, que nos aporta métodos para hacer un heap de manera rápida y eficiente. El código consiste en un método push que añade un elemento al heap, y en el caso de que el número de elementos sea mayor que el cutoff, realiza un pop para eliminar el elemento de menor valor. Esto permite que en el heap siempre queden los n elementos con mayor valor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNkPcUjMpNRn"
   },
   "source": [
    "# Ejercicio 2: Índice en RAM (3pt)\n",
    "\n",
    "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
    "\n",
    "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
    "\n",
    "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVzsIg0Zev7a"
   },
   "source": [
    "## Ejercicio 2.1: Estructura de índice\n",
    "\n",
    "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
    "\n",
    "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "VqSKneeSe2bN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create an Index based on RAM\n",
    "\"\"\"\n",
    "class RAMIndex(Index):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - dir: index directory\n",
    "    \"\"\"\n",
    "    def __init__(self, dir = None):\n",
    "        self.docmap = []\n",
    "        self.modulemap = {}\n",
    "        self.builder_postings = {}\n",
    "        self.loaded_postings = {}\n",
    "        if dir: self.open(dir)\n",
    "            \n",
    "    \"\"\"\n",
    "    Returns all postings for a term\n",
    "    Parameters:\n",
    "        - term: the term to be searched\n",
    "    Return:\n",
    "        - list of tuples (docid, freq)\n",
    "    \"\"\"\n",
    "    def postings(self, term):\n",
    "        return self.loaded_postings[term]\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the terms in the index\n",
    "    Return:\n",
    "        - list of terms\n",
    "    \"\"\"\n",
    "    def all_terms(self):\n",
    "        return self.loaded_postings.keys()\n",
    "\n",
    "    \"\"\"\n",
    "    Saves the builded postings created by the index builder.\n",
    "    Builds the loaded_postings dictionary, needed for the search.\n",
    "    Parameters:\n",
    "        - dir: the directory where the index will be saved\n",
    "    \"\"\"\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "        self.loaded_postings = {}\n",
    "        for term in self.builder_postings.keys():\n",
    "            self.loaded_postings[term] = []\n",
    "            for doc_id in self.builder_postings[term].keys():\n",
    "                self.loaded_postings[term].append((doc_id, self.builder_postings[term][doc_id]))\n",
    "        p = os.path.join(dir, Config.POSTINGS_FILE)\n",
    "        with open(p, 'wb') as f:\n",
    "            pickle.dump(self.loaded_postings, f) \n",
    "\n",
    "        p2 = os.path.join(dir, Config.PATHS_FILE)    \n",
    "        with open(p2, 'wb') as f:\n",
    "            pickle.dump(self.docmap, f)\n",
    "    \n",
    "    \"\"\"\n",
    "    Loads the index from a directory\n",
    "    Parameters:\n",
    "        - dir: the directory where the index is saved\n",
    "    \"\"\"\n",
    "    def open(self, dir):\n",
    "        super().open(dir)    \n",
    "        try:\n",
    "            p = os.path.join(dir, Config.POSTINGS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.loaded_postings = pickle.load(f)\n",
    "            p2 = os.path.join(dir, Config.PATHS_FILE)         \n",
    "            with open(p2, 'rb') as f:\n",
    "                self.docmap = pickle.load(f)\n",
    "\n",
    "        except OSError:\n",
    "            print(\"Error opening index\")\n",
    "            return\n",
    "\n",
    "    \"\"\"\n",
    "    Add info send by the index builder to the postings dictionary\n",
    "    Parameters:\n",
    "        - doc_id: document id\n",
    "        - term: term to be added\n",
    "    \"\"\"\n",
    "    def addToPostings(self, doc_id, term):\n",
    "        if self.builder_postings != {} and term in self.builder_postings.keys():\n",
    "            if doc_id in self.builder_postings[term].keys():\n",
    "                self.builder_postings[term][doc_id]+=1\n",
    "            else:\n",
    "                self.builder_postings[term][doc_id]=1\n",
    "        else:\n",
    "            self.builder_postings[term]={doc_id:1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucORmwfCh4Um"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para implementar un índice con postings, hemos tenido que reconstruir varios de los métodos utilizados por la clase Index para que soporten el uso de postings. El primer paso y más importante ha sido crear un nuevo método addToPostings para poder introducir la información que obtenemos del constructor del índice y transformarla en postings (guardando dos datos para cada término, docuemnto y frecuencia). Sin embargo, para tener mejor acceso a los datos y la estructura de los postings mientras añadimos nuevos desde el builder, addToPostings no utiliza directamente la estructura de tupla que utilizaremos para guardarlos en el índice, sino que los guarda en un diccionario mientras sigamos añadiendo nuevos elementos. Una vez se introduzcan todos los elementos a través del Index Builder, se dará el formato deseado a los datos (en nuestro caso desde el método save).\n",
    "\n",
    "Hemos modificado también los métodos save y open. El método save está modificado para que haga su funcionalidad anterior, y además guarde los paths (que originalmente en la clase de Index no se guardaban pero es algo necesario) y los postings. Para guardar correctamente los postings les damos formato de tuplas. Guardamos los postings listos para su uso en la variable loaded_postings. El método open ha sido modificado para cargar los postings y los paths, además de heredar la funcionalidad de su clase padre.\n",
    "\n",
    "Algunos métodos como postings y all_terms también han sido modificados para que funcionen correctamente con la implementación de postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqbc9ng8e28p"
   },
   "source": [
    "## Ejercicio 2.2: Construcción del índice\n",
    "\n",
    "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "tHQ4UCf5pTw8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create a Builder for a Ram Index\n",
    "\"\"\"\n",
    "class RAMIndexBuilder(Builder):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - dir: index directory\n",
    "    \"\"\"\n",
    "    def __init__(self, dir = None):\n",
    "        super().__init__(dir)\n",
    "        self.dir = dir\n",
    "        self.index = RAMIndex()\n",
    "    \n",
    "    \"\"\"\n",
    "    Commits the index\n",
    "    \"\"\"\n",
    "    def commit(self):\n",
    "        self.index.save(self.dir)\n",
    "\n",
    "    \"\"\"\n",
    "    Indexes a document\n",
    "    Parameters:\n",
    "        - path: the path of the document\n",
    "        - text: the text of the document\n",
    "    \"\"\"\n",
    "    def index_document(self, path, text):\n",
    "        doc_id = self.index.ndocs()\n",
    "        self.index.add_doc(path)\n",
    "        \n",
    "        for term in BasicParser.parse(text):\n",
    "            self.index.addToPostings(doc_id, term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_mxRswZh74N"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para implementar el Index Builder en Ram hemos heredado de la clase Builder. Principalmente dos modificaciones han sido necesarias, primero de todo que al crear el índice sea tipo RAMIndex, y segundo que al indexar el documento en el índice se llame a la función addToPostings explicada en el ejercicio anterior para que se construyan correctamente los postings. Como hemos decidido que toda la funcionalidad se desarroye dentro del índice, el builder no requiere de más modificaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOWgbqZpV01"
   },
   "source": [
    "# Ejercicio 3: Índice en disco* (1pt)\n",
    "\n",
    "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
    "\n",
    "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
    "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
    "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
    "\n",
    "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
    "\n",
    "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "br7yqFrnpZYl"
   },
   "outputs": [],
   "source": [
    "class DiskIndex(Index):\n",
    "    # Your new code here (exercise 3*) #\n",
    "    def __init__(self, dir):\n",
    "        pass\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DiskIndexBuilder(Builder):\n",
    "    # Your new code here (exercise 3*) #\n",
    "    def __init__(self, dir):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTje0viiM9I"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "(por hacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxuclLwpaM-"
   },
   "source": [
    "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
    "\n",
    "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "e3uq565SpfSA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create a Searcher for an index based on proximity score\n",
    "\"\"\"\n",
    "class ProximitySearcher(Searcher):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - index: the index to be used\n",
    "        - parser: the parser to be used\n",
    "    \"\"\"\n",
    "    def __init__(self, index, parser=BasicParser()):\n",
    "        super().__init__(index, parser)\n",
    "    \n",
    "    \"\"\"\n",
    "    Search method\n",
    "    Parameters:\n",
    "        - query: the query to be searched\n",
    "        - cutoff: the number of results to be returned\n",
    "    Return:\n",
    "        - ranking\n",
    "    \"\"\"\n",
    "    def search(self, query, cutoff):\n",
    "\n",
    "        # Construimos un diccionario con cada documento y posiciones de cada termino\n",
    "        terms = self.parser.parse(query)\n",
    "        docs = {}\n",
    "        doc_scores = []\n",
    "        for term in terms:\n",
    "            pos_postings = self.index.positional_postings(term)\n",
    "            for posting in pos_postings:\n",
    "                doc_id = posting[0]\n",
    "                positions = posting[1]\n",
    "                if doc_id not in docs:\n",
    "                    docs[doc_id] = {}\n",
    "                docs[doc_id][term] = positions\n",
    "        \n",
    "        # Si solo hay un termino, añadimos 1 al score por cada vez que aparece\n",
    "        if len(terms) == 1:\n",
    "            scores = {}\n",
    "            postings = self.index.postings(terms[0])\n",
    "            for posting in postings:\n",
    "                if posting[0] not in scores:\n",
    "                    scores[posting[0]] = 0\n",
    "                scores[posting[0]] += posting[1]\n",
    "            ranking = SearchRanking(cutoff)\n",
    "            for doc in scores:\n",
    "                ranking.push(doc, scores[doc])\n",
    "            return ranking\n",
    "            \n",
    "        \n",
    "        # Para cada documento\n",
    "        for doc in docs:\n",
    "            secuencies = []\n",
    "            # Eliminamos los documentos que no tengan todos los terminos\n",
    "            if len(docs[doc]) != len(terms):\n",
    "                doc_scores.append((doc, 0))\n",
    "                continue\n",
    "\n",
    "            # Creamos un puntero para cada termino\n",
    "            pointers = {}\n",
    "            for term in terms:\n",
    "                pointers[term] = 0\n",
    "            \n",
    "            # Escogemos el termino que apunte un puntero con mayor posicion\n",
    "            y_term = terms[0]\n",
    "            y = 0\n",
    "            for term in terms:\n",
    "                if docs[doc][term][pointers[term]] > y:\n",
    "                    y = docs[doc][term][pointers[term]]\n",
    "                    y_term = term\n",
    "            \n",
    "            finish = False\n",
    "            while not finish:\n",
    "                # Avanzamos los punteros (menos y) mientras el numero al que apuntan sea menor que y\n",
    "                for term in terms:\n",
    "                    if term != y_term:\n",
    "                        while docs[doc][term][pointers[term]] < y:\n",
    "                            pointers[term] += 1\n",
    "                            if pointers[term] >= len(docs[doc][term]):\n",
    "                                finish = True\n",
    "                                break\n",
    "                            if docs[doc][term][pointers[term]] > y:\n",
    "                                pointers[term] -= 1\n",
    "                                break\n",
    "\n",
    "                        if finish:\n",
    "                            break\n",
    "\n",
    "                if not finish:\n",
    "                    # x es el puntero que apunta a la posicion mas pequeña\n",
    "                    x_term = y_term\n",
    "                    x = y\n",
    "                    for term in terms:\n",
    "                        if docs[doc][term][pointers[term]] < x:\n",
    "                            x = docs[doc][term][pointers[term]]\n",
    "                            x_term = term\n",
    "                    \n",
    "                    # Añadimos x, y a la lista de posibles\n",
    "                    secuencies.append((x, y))\n",
    "\n",
    "                    # Si x es el ultimo puntero, terminamos\n",
    "                    if pointers[x_term] == len(docs[doc][x_term]) - 1:\n",
    "                        finish = True\n",
    "                    else:\n",
    "                        pointers[x_term] += 1\n",
    "                        y = docs[doc][x_term][pointers[x_term]]\n",
    "                        y_term = x_term\n",
    "            \n",
    "                \n",
    "            \n",
    "            # Calculamos el score con las secuencies\n",
    "            score = 0\n",
    "            if len(secuencies) == 0:\n",
    "                doc_scores.append((doc, 0))\n",
    "                continue\n",
    "\n",
    "            for sec in secuencies:\n",
    "                y = sec[1]\n",
    "                x = sec[0]\n",
    "                score += 1 / (y - x - len(terms) + 2)\n",
    "            doc_scores.append((doc, score))\n",
    "        \n",
    "        # Ordenamos los documentos con ranking\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        for doc in doc_scores:\n",
    "            ranking.push(doc[0],doc[1])\n",
    "        return ranking\n",
    "                \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d4hWTstiOIT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para hacer el buscador con búsqueda proximal hemos seguido al pie de la letra el algoritmo de búsqueda proximal. Para ello utilizamos punteros que nos permiten recorrer las diferentes posiciones en las que está cada uno de los términos de la búsqueda para cada documento (Cabe destacar que el algoritmo se ejecuta para cada documento para obtener su score y así al final poder elegir los mejores utilizando un ranking). Gracias a estos punteros podemos encontrar las diferentes secuencias en las que aparecen los términos que estamos buscando, y así podemos calcular el score.\n",
    "\n",
    "Como la búsqueda proximal solo tiene sentido en el caso de que la búsqueda sea de más de un término, hemos decidido que si la búsqueda es de un solo término, se haga un ranking en función de la frecuencia del término en el documento. Esto no es el método más refinado, pero hemos considerado que es la manera más cercana de hacercarnos al diseño de la búsqueda proximal sin buscar proximidad (ya que lo otro que tiene en cuenta además de la proximidad es la frecuencia de los términos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPPWV7pepf85"
   },
   "source": [
    "# Ejercicio 5: Índice posicional* (1pt)\n",
    "\n",
    "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "zg8MIMpipih1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class of a Positional Index based on RAM\n",
    "\"\"\"\n",
    "class PositionalIndex(RAMIndex):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - dir: index directory\n",
    "    \"\"\"\n",
    "    def __init__(self, dir=None):\n",
    "        self.builder_positions = {}\n",
    "        self.loaded_positions  = {}\n",
    "        super().__init__(dir)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Returns all positional postings for a term\n",
    "    Parameters:\n",
    "        - term: the term to be searched\n",
    "    Return:\n",
    "        - list of tuples (docid, positions)\n",
    "    \"\"\"\n",
    "    def positional_postings(self, term):\n",
    "        return self.loaded_positions[term]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Saves the builded postings created by the index builder.\n",
    "    Builds the loaded_postings dictionary, needed for the search.\n",
    "    Parameters:\n",
    "        - dir: the directory where the index will be saved\n",
    "    \"\"\"\n",
    "    def save(self, dir):\n",
    "        super().save(dir)\n",
    "\n",
    "        self.loaded_positions  = {}\n",
    "        for term in self.builder_positions.keys():\n",
    "            for doc_id in self.builder_positions[term].keys():\n",
    "                positions = self.builder_positions[term][doc_id]\n",
    "                if not term in self.loaded_positions:\n",
    "                    self.loaded_positions[term] = []\n",
    "                self.loaded_positions[term].append(tuple((doc_id, positions))) \n",
    "\n",
    "        p = os.path.join(dir, Config.POSITIONS_FILE)\n",
    "        try:\n",
    "            with open(p, 'wb') as f:\n",
    "                pickle.dump(self.loaded_positions, f) \n",
    "        except Exception as e:\n",
    "            print(\"Error saving the positions file\", e)\n",
    "\n",
    "    \"\"\"\n",
    "    Loads the builded postings created by the index builder.\n",
    "    Parameters:\n",
    "        - dir: the directory for the index to be loaded\n",
    "    \"\"\"\n",
    "    def open(self, dir):\n",
    "        super().open(dir)\n",
    "\n",
    "        try:\n",
    "            p = os.path.join(dir, Config.POSITIONS_FILE)\n",
    "            with open(p, 'rb') as f:\n",
    "                self.loaded_positions = pickle.load(f) \n",
    "        except Exception as e:\n",
    "            print(\"Error opening the positions file\", e)\n",
    "\n",
    "    \"\"\"\n",
    "    Adds the info send by the index builder to the postings dictionary\n",
    "    Parameters:\n",
    "        - doc_id: document id\n",
    "        - term: term to be added\n",
    "        - pos: position of the term in the document\n",
    "    \"\"\"\n",
    "    def addToPostings(self, doc_id, term, pos):\n",
    "        super().addToPostings(doc_id, term)\n",
    "        if term not in self.builder_positions.keys():\n",
    "            self.builder_positions[term] = {}\n",
    "        if doc_id not in self.builder_positions[term]:\n",
    "            self.builder_positions[term][doc_id] = [] \n",
    "        self.builder_positions[term][doc_id].append(pos)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Class to create a Builder for a Positional Index based on RAM\n",
    "\"\"\"\n",
    "class PositionalIndexBuilder(RAMIndexBuilder):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - dir: index directory\n",
    "    \"\"\"\n",
    "    def __init__(self, dir = None):\n",
    "        super().__init__(dir)\n",
    "        self.dir = dir\n",
    "        self.index = PositionalIndex()\n",
    "\n",
    "    \"\"\"\n",
    "    Indexes a document\n",
    "    Parameters:\n",
    "        - path: the path of the document\n",
    "        - text: the text of the document\n",
    "    \"\"\"\n",
    "    def index_document(self, path, text):\n",
    "        doc_id = self.index.ndocs()\n",
    "        self.index.add_doc(path)\n",
    "        for position, term in enumerate(BasicParser.parse(text)):\n",
    "            self.index.addToPostings(doc_id, term, position)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1gukouXiPV3"
   },
   "source": [
    "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
    "\n",
    "Para implementar el índice posicional, hemos utilizado como base el índice en RAM que hemos creado previamente en el ejercicio 2. Esto nos ha permitido trabajar con una base ya funcional y conocida, a partir de la que hemos podido implementar los nuevos índices posicionales. A partir de RAMIndex, hemos implementado los siguientes cambios:\n",
    "\n",
    "En el constructor de índice, hemos reimplementado la función index_document para que, además de su funcionalidad previa, envíe al método addToPostings del índice (donde se desarroya la funcionalidad de guardar los datos para los postings, como está explicado en la documentación del ejercicio 2), además de la id del documento y el término de la petición, la posición donde se encuentra ese término en el documento.\n",
    "\n",
    "Para utilizar la nueva variante del constructor de índice, hemos cambiado la función del índice addToPostings de manera que ahora además de su funcionalidad previa, guarda en un diccionario las localizaciones de cada término. Además, el método save está modificado para que pueda transformar dicho diccionario en tuplas una vez terminada la construcción del índice, y el método open ha sido modificado para poder cargar estas tuplas de postings posicionales otra vez al índice en RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6HbYGn8pjKZ"
   },
   "source": [
    "# Ejercicio 6: PageRank (1pt)\n",
    "\n",
    "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
    "\n",
    "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "_aQE7SBgpk1S"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create a Page Rank Scorer\n",
    "\"\"\"\n",
    "\n",
    "class PagerankDocScorer():\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    Parameters:\n",
    "        - graphfile: file\n",
    "        - r: damping factor\n",
    "        - n_iter: number of iterations\n",
    "    \"\"\"\n",
    "    def __init__(self, graphfile, r, n_iter):\n",
    "        self.r = r \n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        self.links = {}\n",
    "\n",
    "        f = open(graphfile, 'r')\n",
    "        lines = f.readlines()\n",
    "        nodes = []\n",
    "\n",
    "        for l in lines:\n",
    "            link = l.strip().split('\\t')\n",
    "            saliente, entrante = link[0], link[1]\n",
    "\n",
    "            if saliente not in nodes:\n",
    "                nodes.append(saliente)\n",
    "            if entrante not in nodes:\n",
    "                nodes.append(entrante)\n",
    "\n",
    "            if saliente in self.links.keys():\n",
    "                self.links[saliente].append(entrante)\n",
    "            else:\n",
    "                self.links[saliente] = [entrante]\n",
    "        \n",
    "\n",
    "        for n in nodes:\n",
    "            if n not in self.links.keys():\n",
    "                self.links[n] =  nodes\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a ranking of the documents\n",
    "    Parameters:\n",
    "        - cutoff: number of documents to return (maximum)\n",
    "    Returns:\n",
    "        - Ranking\n",
    "    \"\"\"\n",
    "    def rank(self, cutoff):\n",
    "        ranking = SearchRanking(cutoff)\n",
    "        P = {}\n",
    "        Pnext = {}\n",
    "        \n",
    "        for i in self.links.keys():\n",
    "            P[i] = 1/len(self.links.keys())\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "\n",
    "            for i in self.links.keys():\n",
    "                Pnext[i] = self.r/len(self.links.keys())\n",
    "            \n",
    "            for i in self.links.keys():\n",
    "                out = len(self.links[i])\n",
    "                for j in self.links[i]:\n",
    "                    Pnext[j] += (1-self.r)*P[i]/out\n",
    "            \n",
    "            for i in self.links.keys():\n",
    "                P[i] = Pnext[i]\n",
    "        \n",
    "        for node in P:\n",
    "            ranking.push(node, P[node])\n",
    "        \n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ZT7seCiQiT"
   },
   "source": [
    "### Explicación/documentación\n",
    "\n",
    "Para realizar la implementación de PageRank hemos utilizado el algoritmo de PageRank, de modo que al inicializar un objeto de este tipo se aplica el algoritmo, calculando la cantidad de enlaces entrantes y salientes de cada nodo (página), y teniendo en cuenta los sumideros de manera que redirijan a cualquier otro nodo. Así, al ejecutar el método rank, el algoritmo itera sobre los nodos, calculando el PageRank de cada uno de ellos, y devolviendo una lista con los nodos ordenados por PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zXhPtFzon72"
   },
   "source": [
    "# Celda de prueba\n",
    "\n",
    "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que esta celda funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "fTdDacCRn0u6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.023804903030395508 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.017559528350830078 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.01939225196838379 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.0016922950744628906 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Done ( 0.002306699752807617 seconds )\n",
      "\n",
      "DiskIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0014157295227050781 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0010428428649902344 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0009698867797851562 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.0001266002655029297 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 4\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"cc\" in document 0 - ./collections/toy1/d1.txt: 2\n",
      "  Total frequency of word \"cc\" in the collection: 3 occurrences over 2 documents\n",
      "  Docs containing the word 'cc': 2\n",
      "    First two documents: [(0, 2), (2, 1)]\n",
      "Done ( 0.00012135505676269531 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa dd\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0014743804931640625 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.0011792182922363281 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 0.001361846923828125 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "0 \t 0\n",
      "0 \t 2\n",
      "0 \t 1\n",
      "\n",
      "Done ( 0.000598907470703125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0013549327850341797 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005693435668945312 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00035572052001953125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007445812225341797 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005900859832763672 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003464221954345703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0009541511535644531 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005624294281005859 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa dd'\n",
      "1.0 \t ./collections/toy1/d2.txt\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0003228187561035156 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "\n",
      "Done ( 5.2928924560546875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy1/d2.txt\n",
      "\n",
      "Done ( 7.557868957519531e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa dd'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d2.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.866455078125e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "\n",
      "Done ( 3.9577484130859375e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy1/d2.txt\n",
      "\n",
      "Done ( 6.866455078125e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa dd'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d2.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.699562072753906e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa dd'\n",
      "0 \t 0\n",
      "0 \t 2\n",
      "0 \t 1\n",
      "\n",
      "Done ( 9.274482727050781e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0023658275604248047 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004388093948364258 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "2.4757064692351958 \t ./collections/toy1/d1.txt\n",
      "1.9101843771913276 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.003946542739868164 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa'\n",
      "8 \t 0\n",
      "1 \t 2\n",
      "\n",
      "Done ( 0.00039386749267578125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0005106925964355469 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004646778106689453 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0002999305725097656 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0006344318389892578 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0007281303405761719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0004811286926269531 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0011107921600341797 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0009388923645019531 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa'\n",
      "0.7427813527082074 \t ./collections/toy1/d1.txt\n",
      "0.5773502691896258 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.0009377002716064453 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa'\n",
      "\n",
      "Done ( 7.963180541992188e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 9.942054748535156e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 8.273124694824219e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa'\n",
      "\n",
      "Done ( 5.602836608886719e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00010204315185546875 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 6.127357482910156e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa'\n",
      "8 \t 0\n",
      "1 \t 2\n",
      "\n",
      "Done ( 0.00010156631469726562 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.022556543350219727 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.020670652389526367 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.018912315368652344 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0009872913360595703 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0016314983367919922 seconds )\n",
      "\n",
      "DiskIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0005807876586914062 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0006384849548339844 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 38\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5.0 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 0.0004761219024658203 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 6.794929504394531e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 2\n",
      "Vocabulary size: 56\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy2/example.txt: 4\n",
      "  Total frequency of word \"aa\" in the collection: 5 occurrences over 2 documents\n",
      "  Docs containing the word 'aa': 2\n",
      "    First two documents: [(0, 4), (1, 1)]\n",
      "Done ( 6.0558319091796875e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0025458335876464844 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0016698837280273438 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.003256559371948242 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "2.333333333333333 \t 0\n",
      "0 \t 1\n",
      "\n",
      "Done ( 0.0006341934204101562 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0011229515075683594 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000782012939453125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005509853363037109 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008699893951416016 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007097721099853516 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00032973289489746094 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006844997406005859 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006778240203857422 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005857944488525391 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "\n",
      "Done ( 0.0001220703125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00010752677917480469 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.535385131835938e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "\n",
      "Done ( 4.9591064453125e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.678436279296875e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 8.249282836914062e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa cc'\n",
      "2.333333333333333 \t 0\n",
      "0 \t 1\n",
      "\n",
      "Done ( 0.00022935867309570312 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.003690958023071289 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.0029985904693603516 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.9361992914246073 \t ./collections/toy2/example.txt\n",
      "\n",
      "Done ( 0.002061128616333008 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "2.0 \t 0\n",
      "0 \t 1\n",
      "\n",
      "Done ( 0.0007829666137695312 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.002065896987915039 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014095306396484375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006194114685058594 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.000499725341796875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0005645751953125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00037670135498046875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006573200225830078 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006215572357177734 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "0.902184145803128 \t ./collections/toy2/example.txt\n",
      "0.036794545335038994 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0008518695831298828 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "\n",
      "Done ( 7.033348083496094e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 7.152557373046875e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 6.604194641113281e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "\n",
      "Done ( 3.790855407714844e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 6.866455078125e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00013589859008789062 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'bb aa'\n",
      "2.0 \t 0\n",
      "0 \t 1\n",
      "\n",
      "Done ( 0.00015354156494140625 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 2 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.023693323135375977 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.026062726974487305 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.021924734115600586 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.001119852066040039 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/toy1/\n",
      "Collection: ./collections/toy2/\n",
      "Done ( 0.0019698143005371094 seconds )\n",
      "\n",
      "DiskIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0011353492736816406 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0016498565673828125 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 39\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14.0 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0016758441925048828 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0001366138458251953 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 6\n",
      "Vocabulary size: 57\n",
      "  Frequency of word \"aa\" in document 0 - ./collections/toy1/d1.txt: 8\n",
      "  Total frequency of word \"aa\" in the collection: 14 occurrences over 4 documents\n",
      "  Docs containing the word 'aa': 4\n",
      "    First two documents: [(0, 8), (2, 1)]\n",
      "Done ( 0.0001499652862548828 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for aa cc\n",
      "  WhooshSearcher with index WhooshIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.005774259567260742 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.007569074630737305 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "4.391396809311482 \t ./collections/toy1/d1.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004108428955078125 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "3.0 \t 0\n",
      "2.333333333333333 \t 4\n",
      "0 \t 2\n",
      "0 \t 5\n",
      "\n",
      "Done ( 0.0006306171417236328 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0017423629760742188 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.090073558454821 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010192394256591797 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0010998249053955078 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0024862289428710938 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.090073558454821 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0017287731170654297 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0009300708770751953 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0023941993713378906 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.090073558454821 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0015156269073486328 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'aa cc'\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "1.0302803432425018 \t ./collections/toy1/d1.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0007536411285400391 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "\n",
      "Done ( 8.296966552734375e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00012969970703125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'aa cc'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 9.775161743164062e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "\n",
      "Done ( 4.9591064453125e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00011301040649414062 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'aa cc'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0001513957977294922 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'aa cc'\n",
      "3.0 \t 0\n",
      "2.333333333333333 \t 4\n",
      "0 \t 2\n",
      "0 \t 5\n",
      "\n",
      "Done ( 0.0004277229309082031 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for bb aa\n",
      "  WhooshSearcher with index WhooshIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.00537419319152832 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.006220102310180664 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.799979765451932 \t ./collections/toy1/d1.txt\n",
      "4.623492062680375 \t ./collections/toy2/example.txt\n",
      "3.9373053181875237 \t ./collections/toy1/d3.txt\n",
      "\n",
      "Done ( 0.004500389099121094 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "4.5 \t 0\n",
      "2.0 \t 4\n",
      "0 \t 2\n",
      "0 \t 5\n",
      "\n",
      "Done ( 0.0005109310150146484 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.001894235610961914 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.090073558454821 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.002232789993286133 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0014948844909667969 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0019404888153076172 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.090073558454821 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.001432657241821289 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0006945133209228516 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0017595291137695312 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.090073558454821 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0019032955169677734 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'bb aa'\n",
      "1.2567295346540424 \t ./collections/toy1/d1.txt\n",
      "1.0900735584548211 \t ./collections/toy2/example.txt\n",
      "1.0555682311333752 \t ./collections/toy1/d3.txt\n",
      "0.059960347471425264 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.001214742660522461 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "\n",
      "Done ( 0.00010442733764648438 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00016736984252929688 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'bb aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00024080276489257812 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "\n",
      "Done ( 0.0002307891845703125 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.00028705596923828125 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'bb aa'\n",
      "0 \t ./collections/toy1/d1.txt\n",
      "0 \t ./collections/toy1/d3.txt\n",
      "0 \t ./collections/toy2/example.txt\n",
      "0 \t ./collections/toy2/hamlet.txt\n",
      "\n",
      "Done ( 0.0002944469451904297 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'bb aa'\n",
      "4.5 \t 0\n",
      "2.0 \t 4\n",
      "0 \t 2\n",
      "0 \t 5\n",
      "\n",
      "Done ( 0.00028705596923828125 seconds )\n",
      "\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 4.184134006500244 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.384700298309326 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.9295730590820312 seconds )\n",
      "\n",
      "Building index with <class '__main__.RAMIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 2.6849167346954346 seconds )\n",
      "\n",
      "DiskIndexBuilder still not implemented\n",
      "Building index with <class '__main__.PositionalIndexBuilder'>\n",
      "Collection: ./collections/urls.txt\n",
      "Done ( 3.8455348014831543 seconds )\n",
      "\n",
      "DiskIndex still not implemented (index)\n",
      "Reading index with <class '__main__.WhooshIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6119\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.020489931106567383 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshForwardIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6119\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.019640684127807617 seconds )\n",
      "\n",
      "Reading index with <class '__main__.WhooshPositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6119\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 5\n",
      "  Total frequency of word \"wikipedia\" in the collection: 24.0 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 5), (1, 13)]\n",
      "Done ( 0.024863719940185547 seconds )\n",
      "\n",
      "Reading index with <class '__main__.RAMIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6009\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 9.560585021972656e-05 seconds )\n",
      "\n",
      "Reading index with <class '__main__.PositionalIndex'>\n",
      "Collection size: 3\n",
      "Vocabulary size: 6009\n",
      "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n",
      "  Total frequency of word \"wikipedia\" in the collection: 28 occurrences over 3 documents\n",
      "  Docs containing the word 'wikipedia': 3\n",
      "    First two documents: [(0, 6), (1, 14)]\n",
      "Done ( 8.940696716308594e-05 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for information probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004547834396362305 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.005224704742431641 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004724740982055664 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.14454497074230405 \t 2\n",
      "0.007837626241515336 \t 0\n",
      "0.0029239766081871343 \t 1\n",
      "\n",
      "Done ( 0.0007066726684570312 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0012187957763671875 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0013606548309326172 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006308555603027344 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037508010864257812 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0032279491424560547 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003719329833984375 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006098747253417969 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006973743438720703 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00031757354736328125 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'information probability'\n",
      "\n",
      "Done ( 3.933906555175781e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 6.842613220214844e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 6.389617919921875e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "\n",
      "Done ( 3.0517578125e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 6.508827209472656e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'information probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 6.103515625e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'information probability'\n",
      "0.0883303189171283 \t 2\n",
      "0.005036226176061505 \t 0\n",
      "0.002036659877800407 \t 1\n",
      "\n",
      "Done ( 0.000461578369140625 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for probability information\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.006723880767822266 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.006995439529418945 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.007149934768676758 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.14454497074230405 \t 2\n",
      "0.007837626241515336 \t 0\n",
      "0.0029239766081871343 \t 1\n",
      "\n",
      "Done ( 0.0009205341339111328 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0015494823455810547 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0015993118286132812 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007627010345458984 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0037279129028320312 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0031576156616210938 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004680156707763672 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007283687591552734 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0010156631469726562 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0004718303680419922 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'probability information'\n",
      "\n",
      "Done ( 8.058547973632812e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 8.58306884765625e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 7.605552673339844e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "\n",
      "Done ( 3.7670135498046875e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 0.00012874603271484375 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'probability information'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 0.0001380443572998047 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'probability information'\n",
      "0.0883303189171283 \t 2\n",
      "0.005036226176061505 \t 0\n",
      "0.002036659877800407 \t 1\n",
      "\n",
      "Done ( 0.00018739700317382812 seconds )\n",
      "\n",
      "------------------------------\n",
      "Checking search results for higher probability\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004094362258911133 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.003570079803466797 seconds )\n",
      "\n",
      "  WhooshSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0033195018768310547 seconds )\n",
      "\n",
      "  ProximitySearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "1.082414839067712 \t 2\n",
      "0.061180813087027344 \t 0\n",
      "0.0003288391976323578 \t 1\n",
      "\n",
      "Done ( 0.00066375732421875 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0011487007141113281 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.02672807672401191 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0012140274047851562 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.00036644935607910156 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0019843578338623047 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.02672807672401191 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0021593570709228516 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshForwardIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003895759582519531 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007083415985107422 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.02672807672401191 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007128715515136719 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index WhooshPositionalIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0003464221954345703 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "\n",
      "Done ( 3.910064697265625e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 7.510185241699219e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 6.914138793945312e-05 seconds )\n",
      "\n",
      "  SlowVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "\n",
      "Done ( 3.24249267578125e-05 seconds )\n",
      "\n",
      "  TermBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 7.033348083496094e-05 seconds )\n",
      "\n",
      "  DocBasedVSMSearcher with index PositionalIndex for query 'higher probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 6.580352783203125e-05 seconds )\n",
      "\n",
      "  ProximitySearcher with index PositionalIndex for query 'higher probability'\n",
      "1.0413490051969339 \t 2\n",
      "0.04158083011974832 \t 0\n",
      "0.00021944261575597982 \t 1\n",
      "\n",
      "Done ( 0.00029540061950683594 seconds )\n",
      "\n",
      "----------------------------\n",
      "Testing index performance on ['./collections/urls.txt'] document collection\n",
      "  Build time...\n",
      "\tWhooshIndex: 3.171753168106079 seconds ---\n",
      "\tWhooshForwardIndex: 5.965422868728638 seconds ---\n",
      "\tWhooshPositionalIndex: 3.479720115661621 seconds ---\n",
      "\tRAMIndex: 2.3391761779785156 seconds ---\n",
      "DiskIndexBuilder still not implemented\n",
      "  Load time...\n",
      "\tWhooshIndex: 0.001489877700805664 seconds ---\n",
      "\tWhooshForwardIndex: 0.0009527206420898438 seconds ---\n",
      "\tWhooshPositionalIndex: 0.0008525848388671875 seconds ---\n",
      "\tRAMIndex: 0.0034513473510742188 seconds ---\n",
      "DiskIndex still not implemented\n",
      "  Disk space...\n",
      "\tWhooshIndex: 1101097 space ---\n",
      "\tWhooshForwardIndex: 1179272 space ---\n",
      "\tWhooshPositionalIndex: 1260195 space ---\n",
      "\tRAMIndex: 128074 space ---\n",
      "\tDiskIndex: 0 space ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'information probability'\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'information probability'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0033965110778808594 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.004815340042114258 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007150173187255859 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.000736236572265625 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'information probability'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006761550903320312 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0006949901580810547 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'information probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 7.963180541992188e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00010061264038085938 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'probability information'\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'probability information'\n",
      "2.9281078841007577 \t https://en.wikipedia.org/wiki/Entropy\n",
      "2.7486633271063745 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.1055615808722834 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.004235029220581055 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.005440950393676758 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206386 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007414817810058594 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0007641315460205078 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'probability information'\n",
      "0.023281466219790506 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.017479282471427333 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.009644386806206384 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006694793701171875 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0006890296936035156 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'probability information'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 8.082389831542969e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.0001010894775390625 seconds ---\n",
      "----------------------------\n",
      "Testing search performance on ['./collections/urls.txt'] document collection with query: 'higher probability'\n",
      "DiskIndex still not implemented\n",
      "  WhooshSearcher with index WhooshIndex for query 'higher probability'\n",
      "2.874812874380949 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "2.532526727742532 \t https://en.wikipedia.org/wiki/Entropy\n",
      "1.7382513569073423 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0031790733337402344 seconds )\n",
      "\n",
      "--- Whoosh on Whoosh 0.004412412643432617 seconds ---\n",
      "  SlowVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.026728076724011913 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0006754398345947266 seconds )\n",
      "\n",
      "--- SlowVSM on Whoosh 0.0006966590881347656 seconds ---\n",
      "  TermBasedVSMSearcher with index WhooshIndex for query 'higher probability'\n",
      "0.02672807672401191 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0.012312006998916738 \t https://en.wikipedia.org/wiki/Entropy\n",
      "0.005762460840449962 \t https://en.wikipedia.org/wiki/Bias\n",
      "\n",
      "Done ( 0.0007035732269287109 seconds )\n",
      "\n",
      "--- TermVSM on Whoosh 0.0007226467132568359 seconds ---\n",
      "  TermBasedVSMSearcher with index RAMIndex for query 'higher probability'\n",
      "0 \t https://en.wikipedia.org/wiki/Bias\n",
      "0 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n",
      "0 \t https://en.wikipedia.org/wiki/Entropy\n",
      "\n",
      "Done ( 8.130073547363281e-05 seconds )\n",
      "\n",
      "--- TermVSM on RAM 0.00010347366333007812 seconds ---\n",
      "=================================================================\n",
      "Testing indices and search on 1 collections\n",
      "Building index with <class '__main__.WhooshBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 69.01227688789368 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshForwardBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n",
      "Done ( 90.77012634277344 seconds )\n",
      "\n",
      "Building index with <class '__main__.WhooshPositionalBuilder'>\n",
      "Collection: ./collections/docs1k.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def test_collection(collection_paths: list, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
    "    print(\"=================================================================\")\n",
    "    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n",
    "\n",
    "    # We now test building different implementations of an index\n",
    "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_paths)\n",
    "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_paths)\n",
    "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_paths)\n",
    "    try:\n",
    "        test_build(RAMIndexBuilder(index_path + \"ram\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(DiskIndexBuilder(index_path + \"disk\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_paths)\n",
    "    except NotImplementedError:\n",
    "        print(\"PositionalIndexBuilder still not implemented\")\n",
    "\n",
    "    def catch_index(func, name, *args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except NotImplementedError:\n",
    "            print(name + \" still not implemented (index)\")\n",
    "            return None\n",
    "\n",
    "    # We now inspect all the implementations\n",
    "    indices = [\n",
    "            WhooshIndex(index_path + \"whoosh\"),\n",
    "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
    "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
    "            catch_index(lambda: RAMIndex(index_path + \"ram\"), \"RAMIndex\"),\n",
    "            catch_index(lambda: DiskIndex(index_path + \"disk\"), \"DiskIndex\"),\n",
    "            catch_index(lambda: PositionalIndex(index_path + \"pos\"), \"PositionalIndex\"),\n",
    "            ]\n",
    "    for index in indices:\n",
    "        if index:\n",
    "            test_read(index, word)\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"------------------------------\")\n",
    "        print(\"Checking search results for %s\" % (query))\n",
    "        # Whoosh searcher can only work with its own indices\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
    "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        try:\n",
    "            test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher still not implemented\")\n",
    "        for index in indices:\n",
    "            if index:\n",
    "                # our searchers should work with any other index\n",
    "                test_search(SlowVSMSearcher(index), index, query, 5)\n",
    "                try:\n",
    "                    test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"TermBasedVSMSearcher still not implemented\")\n",
    "                try:\n",
    "                    test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
    "                except NotImplementedError:\n",
    "                    print(\"DocBasedVSMSearcher still not implemented\")\n",
    "        try:\n",
    "            test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
    "        except NotImplementedError:\n",
    "            print(\"ProximitySearcher or PositionalIndex still not implemented\")\n",
    "\n",
    "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
    "    indices = list()\n",
    "\n",
    "    if analyse_performance:\n",
    "        # let's analyse index performance\n",
    "        test_index_performance(collection_paths, index_path)\n",
    "        # let's analyse search performance\n",
    "        for query in queries:\n",
    "            test_search_performance(collection_paths, index_path, query, 5)\n",
    "\n",
    "def test_build(builder, collections: list):\n",
    "    stamp = time.time()\n",
    "    print(\"Building index with\", type(builder))\n",
    "    for collection in collections:\n",
    "        print(\"Collection:\", collection)\n",
    "        # this function should index the received collection and add it to the index\n",
    "        builder.build(collection)\n",
    "    # when we commit, the information in the index becomes persistent\n",
    "    # we can also save any extra information we may need\n",
    "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
    "    builder.commit()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def test_read(index, word):\n",
    "    stamp = time.time()\n",
    "    print(\"Reading index with\", type(index))\n",
    "    print(\"Collection size:\", index.ndocs())\n",
    "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
    "    # more tests\n",
    "    doc_id = 0\n",
    "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
    "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
    "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
    "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def test_search (engine, index, query, cutoff):\n",
    "    stamp = time.time()\n",
    "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
    "    for path, score in engine.search(query, cutoff):\n",
    "        print(score, \"\\t\", path)\n",
    "    print()\n",
    "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
    "    print()\n",
    "\n",
    "def disk_space(index_path: str) -> int:\n",
    "    space = 0\n",
    "    if os.path.isdir(index_path):\n",
    "        for f in os.listdir(index_path):\n",
    "            p = os.path.join(index_path, f)\n",
    "            if os.path.isfile(p):\n",
    "                space += os.path.getsize(p)\n",
    "    return space\n",
    "\n",
    "def test_index_performance (collection_paths: list, base_index_path: str):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing index performance on \" + str(collection_paths) + \" document collection\")\n",
    "\n",
    "    print(\"  Build time...\")\n",
    "    start_time = time.time()\n",
    "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
    "    for collection_path in collection_paths:\n",
    "        b.build(collection_path)\n",
    "    b.commit()\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = RAMIndexBuilder(base_index_path + \"ram\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndexBuilder still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        b = DiskIndexBuilder(base_index_path + \"disk\")\n",
    "        for collection_path in collection_paths:\n",
    "            b.build(collection_path)\n",
    "        b.commit()\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndexBuilder still not implemented\")\n",
    "\n",
    "    print(\"  Load time...\")\n",
    "    start_time = time.time()\n",
    "    WhooshIndex(base_index_path + \"whoosh\")\n",
    "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
    "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
    "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        RAMIndex(base_index_path + \"ram\")\n",
    "        print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        DiskIndex(base_index_path + \"disk\")\n",
    "        print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "\n",
    "    print(\"  Disk space...\")\n",
    "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
    "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
    "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
    "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
    "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
    "\n",
    "\n",
    "def test_search_performance (collection_paths: list, base_index_path: str, query: str, cutoff: int):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Testing search performance on \" + str(collection_paths) + \" document collection with query: '\" + query + \"'\")\n",
    "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
    "    try:\n",
    "        ram_index = RAMIndex(base_index_path + \"ram\")\n",
    "    except NotImplementedError:\n",
    "        print(\"RAMIndex still not implemented\")\n",
    "        ram_index = None\n",
    "    try:\n",
    "        disk_index = DiskIndex(base_index_path + \"disk\")\n",
    "    except NotImplementedError:\n",
    "        print(\"DiskIndex still not implemented\")\n",
    "        disk_index = None\n",
    "\n",
    "    start_time = time.time()\n",
    "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
    "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # let's test some combinations of ranking + index implementations\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
    "        print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if ram_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
    "            print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"TermBasedVSMSearcher still not implemented\")\n",
    "\n",
    "    try:\n",
    "        if disk_index:\n",
    "            start_time = time.time()\n",
    "            test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
    "            print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"DocBasedVSMSearcher still not implemented\")\n",
    "\n",
    "def test_pagerank(graphs_root_dir, cutoff):\n",
    "    print(\"----------------------------\")\n",
    "    # we separate this function because it cannot work with all the collections\n",
    "    print(\"Testing PageRank\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
    "            print(score, \"\\t\", path)\n",
    "        print()\n",
    "        print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
    "    except NotImplementedError:\n",
    "        print(\"PagerankDocScorer still not implemented\")\n",
    "\n",
    "\n",
    "index_root_dir = \"./index/\"\n",
    "collections_root_dir = \"./collections/\"\n",
    "\n",
    "\n",
    "test_collection ([collections_root_dir + \"toy1/\"], index_root_dir + \"toy1/\", \"cc\", [\"aa dd\", \"aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy2/\"], index_root_dir + \"toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"toy1/\", collections_root_dir + \"toy2/\"], index_root_dir + \"toys/\", \"aa\", [\"aa cc\", \"bb aa\"], False)\n",
    "test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], True)\n",
    "test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
    "test_collection ([collections_root_dir + \"toy2/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"three_collections/\", \"seat\", [\"obama family tree\"], True)\n",
    "#test_collection ([collections_root_dir + \"docs10k.zip\"], index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], False)\n",
    "test_pagerank(\"./collections/\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5JhJJSFiSl5"
   },
   "source": [
    "### Resumen de coste y rendimiento\n",
    "\n",
    "Hay que analizar las **diferencias de rendimiento** observadas entre las diferentes implementaciones que se han creado y probado para cada componente.\n",
    "\n",
    "En concreto, hay que reportar tiempo de indexado, consumo máximo de RAM y espacio en disco al construir el índice, y el tiempo de carga y consumo máximo de RAM al cargar el índice para cada una de las colecciones utilizadas.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "|      | Construcción | del | índice | Carga del | índice |\n",
    "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
    "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
    "| toy1 | | | | | |\n",
    "| toy2 | | | | | |\n",
    "| toys | | | | | |\n",
    "| 1K | | | | | |\n",
    "| 10K | | | | | |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Enunciado P2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
