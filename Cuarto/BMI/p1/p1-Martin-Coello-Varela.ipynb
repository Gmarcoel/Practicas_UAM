{"cells":[{"cell_type":"markdown","metadata":{"id":"PMuz6soJxEmP"},"source":["### **B√∫squeda y Miner√≠a de Informaci√≥n 2022-23**\n","### Universidad Aut√≥noma de Madrid, Escuela Polit√©cnica Superior\n","### Grado en Ingenier√≠a Inform√°tica, 4¬∫ curso\n","# **Implementaci√≥n de un motor de b√∫squeda**\n","\n","Fechas:\n","\n","* Comienzo: martes 7 / jueves 9 de febrero\n","* Entrega: martes 21 / jueves 23 de febrero (14:00)"]},{"cell_type":"markdown","metadata":{"id":"t-bdu5nO581M"},"source":["# Introducci√≥n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5T91gVNMmMxO"},"source":["## Autores\n","\n","Guillermo Mart√≠n-Coello Juarez & Daniel Varela Sanchez"]},{"cell_type":"markdown","metadata":{"id":"xlYtzLZp6KwJ"},"source":["## Objetivos\n","\n","Los objetivos de esta pr√°ctica son:\n","\n","* La iniciaci√≥n a la implementaci√≥n de un motor de b√∫squeda.\n","*\tUna primera comprensi√≥n de los elementos b√°sicos necesarios para implementar un motor completo.\n","*\tLa iniciaci√≥n al uso de la librer√≠a [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) en Python para la creaci√≥n y utilizaci√≥n de √≠ndices, funcionalidades de b√∫squeda en texto.\n","*\tLa iniciaci√≥n a la implementaci√≥n de una funci√≥n de r√°nking sencilla.\n","\n","Los documentos que se indexar√°n en esta pr√°ctica, y sobre los que se realizar√°n consultas de b√∫squeda ser√°n documentos HTML, que deber√°n ser tratados para extraer y procesar el texto contenido en ellos. \n","\n","La pr√°ctica plantea como punto de partida una peque√±a API general sencilla (y cuyo uso se puede ver en un programa de prueba que se encuentra al final del enunciado), que pueda implementarse de diferentes maneras, como as√≠ se har√° en esta pr√°ctica y las siguientes. A modo de toma de contacto y arranque de la asignatura, en esta primera pr√°ctica se completar√° una implementaci√≥n de la API utilizando Whoosh, con lo que resultar√° bastante trivial la soluci√≥n (en cuanto a la cantidad de c√≥digo a escribir). En la siguiente pr√°ctica el estudiante desarrollar√° sus propias implementaciones, sustituyendo el uso de Whoosh que vamos a hacer en esta primera pr√°ctica.\n","\n","En t√©rminos de operaciones propias de un motor de b√∫squeda, en esta pr√°ctica el estudiante se encargar√° fundamentalmente de:\n","\n","a) En el proceso de indexaci√≥n: recorrer los documentos de texto de una colecci√≥n dada, eliminar del contenido posibles marcas tales como html, y enviar el texto a indexar por parte de Whoosh. \n","\n","b) En el proceso de responder consultas: implementar una primera versi√≥n sencilla de una o dos funciones de r√°nking en el modelo vectorial, junto con alguna peque√±a estructura auxiliar."]},{"cell_type":"markdown","metadata":{"id":"_AjDofIw6Ns6"},"source":["## Material proporcionado\n","\n","Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n","\n","*\tVarias clases e interfaces Python (mayormente incompletas) a lo largo de este *notebook*, desde las que el estudiante partir√° para completar c√≥digo e integrar√° con ellas las suyas propias. \n","La celda de prueba *al final de este notebook* implementa un programa que deber√° funcionar con el c√≥digo a implementar por el estudiante. Adem√°s, se proporciona a continuaci√≥n una celda con c√≥digo ejemplo que ilustra las funciones m√°s √∫tiles de la API de Whoosh.\n","*\tUna peque√±a colecci√≥n <ins>docs1k.zip</ins> con aproximadamente 1.000 documentos HTML, y un peque√±o fichero <ins>urls.txt</ins>. Ambas representan colecciones de prueba para depurar las implementaciones y comprobar su correcci√≥n.\n","*\tUn documento de texto <ins>output.txt</ins> con la salida est√°ndar que deber√° producir la ejecuci√≥n de la celda de prueba (salvo los tiempos de ejecuci√≥n que pueden cambiar, aunque la tendencia en cuanto a qu√© m√©todos tardan m√°s o menos deber√≠a cumplirse)."]},{"cell_type":"markdown","metadata":{"id":"q9udSskn_eY7"},"source":["## Ejemplo API Whoosh\n","\n","En la siguiente celda de c√≥digo se incluyen varios ejemplos para comprobar c√≥mo usar la API de la librer√≠a *Whoosh*."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"iTYZA7PYmMxR","outputId":"525f276d-fc44-45a4-8989-c6424209861a"},"outputs":[{"ename":"URLError","evalue":"<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)>","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1454\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m-> 1454\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mwrap_socket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock,\n\u001b[1;32m   1455\u001b[0m                                       server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:512\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    507\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    510\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    513\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    514\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    515\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    516\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    517\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    518\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    519\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    520\u001b[0m     )\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1070\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1070\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1071\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1341\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1342\u001b[0m \u001b[39mfinally\u001b[39;00m:\n","\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 57\u001b[0m\n\u001b[1;32m     51\u001b[0m urls \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Simpson\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms_paradox\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Bias\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Entropy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mindex/whoosh/example/urls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m whooshexample_buildindex(\u001b[39mdir\u001b[39;49m, urls)\n\u001b[1;32m     58\u001b[0m whooshexample_search(\u001b[39mdir\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m whooshexample_examine(\u001b[39mdir\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mwhooshexample_buildindex\u001b[0;34m(dir, urls)\u001b[0m\n\u001b[1;32m     18\u001b[0m writer \u001b[39m=\u001b[39m whoosh\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mcreate_in(\u001b[39mdir\u001b[39m, Document)\u001b[39m.\u001b[39mwriter()\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m urls:\n\u001b[0;32m---> 20\u001b[0m     writer\u001b[39m.\u001b[39madd_document(path\u001b[39m=\u001b[39murl, content\u001b[39m=\u001b[39mBeautifulSoup(urlopen(url)\u001b[39m.\u001b[39mread(), \u001b[39m\"\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     21\u001b[0m writer\u001b[39m.\u001b[39mcommit()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m:\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)>"]}],"source":["# Whoosh API\n","import whoosh\n","from whoosh.fields import Schema, TEXT, ID\n","from whoosh.formats import Format\n","from whoosh.qparser import QueryParser\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import os, os.path\n","import shutil\n","\n","Document = Schema(\n","        path=ID(stored=True),\n","        content=TEXT(vector=Format))\n","\n","def whooshexample_buildindex(dir, urls):\n","    if os.path.exists(dir): shutil.rmtree(dir)\n","    os.makedirs(dir)\n","    writer = whoosh.index.create_in(dir, Document).writer()\n","    for url in urls:\n","        writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n","    writer.commit()\n","\n","def whooshexample_search(dir, query):\n","    index = whoosh.index.open_dir(dir)\n","    searcher = index.searcher()\n","    qparser = QueryParser(\"content\", schema=index.schema)\n","    print(\"Search results for '\", query, \"'\")\n","    for docid, score in searcher.search(qparser.parse(query)).items():\n","        print(score, \"\\t\", index.reader().stored_fields(docid)['path'])\n","    print()\n","\n","def whooshexample_examine(dir, term, docid, n):\n","    reader = whoosh.index.open_dir(dir).reader()\n","    print(\"Total nr. of documents in the collection:\", reader.doc_count())\n","    print(\"Total frequency of '\", term, \"':\", reader.frequency(\"content\", term))\n","    print(\"Nr. documents containing '\", term, \"':\", reader.doc_frequency(\"content\", term))\n","    for p in reader.postings(\"content\", term).items_as(\"frequency\") if reader.doc_frequency(\"content\", term) > 0 else []:\n","        print(\"\\tFrequency of '\", term, \"' in document\", p[0], \":\", p[1])\n","    raw_vec = reader.vector(docid, \"content\")\n","    raw_vec.skip_to(term)\n","    if raw_vec.id() == term:\n","        print(\"Frequency of '\", raw_vec.id(), \"' in document\", docid, reader.stored_fields(docid)['path'], \":\", raw_vec.value_as(\"frequency\"))\n","    else:\n","        print(\"Term '\", term, \"' not found in document\", docid)\n","    print(\"Top\", n, \"most frequent terms in document\", docid, reader.stored_fields(docid)['path']) \n","    vec = reader.vector(docid, \"content\").items_as(\"frequency\")\n","    for p in sorted(vec, key=lambda x: x[1], reverse=True)[0:n]:\n","        print(\"\\t\", p)\n","    print()\n","\n","urls = [\"https://en.wikipedia.org/wiki/Simpson's_paradox\", \n","        \"https://en.wikipedia.org/wiki/Bias\",\n","        \"https://en.wikipedia.org/wiki/Entropy\"]\n","\n","dir = \"index/whoosh/example/urls\"\n","\n","whooshexample_buildindex(dir, urls)\n","whooshexample_search(dir, \"probability\")\n","whooshexample_examine(dir, \"probability\", 0, 5)"]},{"cell_type":"markdown","metadata":{"id":"2ciKzD4D6Xn6"},"source":["## Calificaci√≥n\n","\n","Esta pr√°ctica se calificar√° con una puntuaci√≥n de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado.  \n","\n","El peso de la nota de esta pr√°ctica en la calificaci√≥n final de pr√°cticas es del **20%**.\n","\n","La calificaci√≥n se basar√° en a) el **n√∫mero** de ejercicios realizados y b) la **calidad** de los mismos. \n","La puntuaci√≥n que se indica en cada apartado es orientativa, en principio se aplicar√° tal cual se refleja pero podr√° matizarse por criterios de buen sentido si se da el caso.\n","\n","Para dar por v√°lida la realizaci√≥n de un ejercicio, el c√≥digo deber√° funcionar (a la primera) **sin ninguna modificaci√≥n**. El profesor comprobar√° este aspecto ejecutando la celda de prueba as√≠ como otras pruebas adicionales."]},{"cell_type":"markdown","metadata":{"id":"nnln3zQV6anE"},"source":["## Entrega\n","\n","La entrega consistir√° en un √∫nico fichero tipo *notebook* donde se incluir√°n todas las **implementaciones** solicitadas en cada ejercicio, as√≠ como una explicaci√≥n de cada uno a modo de **memoria**. Si se necesita entregar alg√∫n fichero adicional (por ejemplo, im√°genes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir ser√° **bmi-p1-XX**, donde XX debe sustituirse por el n√∫mero de pareja (01, 02, ..., 10, ...).\n","\n","En concreto, se debe documentar:\n","\n","- Qu√© version(es) del modelo vectorial se ha(n) implementado en el ejercicio 2.\n","- C√≥mo se ha conseguido colocar un documento en la primera posici√≥n de r√°nking, para cada buscador implementado en el ejercicio 2.\n","- El trabajo realizado en el ejercicio 3. \n","- Y cualquier otro aspecto que el estudiante considere oportuno destacar.\n"]},{"cell_type":"markdown","metadata":{"id":"6GtUWMA76bP0"},"source":["## Indicaciones\n","\n","Se podr√°n definir clases adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar c√≥digo. Y el estudiante podr√° utilizar o no el software que se le proporciona, con la siguiente limitaci√≥n: \n","\n","*\tNo deber√° editarse el c√≥digo proporcionado m√°s all√° de donde se indica expl√≠citamente.\n","*\t**La celda de prueba deber√° ejecutar** correctamente sin ninguna modificaci√≥n."]},{"cell_type":"markdown","metadata":{"id":"gICjfJ-B6g0Y"},"source":["# Ejercicio 1: Implementaci√≥n basada en Whoosh\n","\n","Implementar las clases y m√≥dulos necesarios para que la celda de prueba funcione. Se deja al estudiante deducir alguna de las relaciones jer√°rquicas entre las clases Python."]},{"cell_type":"markdown","metadata":{"id":"m18qmDwAzANn"},"source":["## Ejercicio 1.1: Indexaci√≥n (3.5pt)\n","\n","Definir las siguientes clases:\n","\n","* Index: clase general (no depende de Whoosh) y que encapsule los m√©todos necesarios para que funcione la celda de prueba que se encuentra al final del enunciado.\n","* Builder: clase general (no depende de Whoosh) que permite construir un √≠ndice (a trav√©s del m√©todo Builder.build()), tal y como se llama desde la celda de prueba entregada.\n","* WhooshIndex: clase que cumpla con la interfaz definida en *Index* usando la librer√≠a de whoosh.\n","* WhooshBuilder: clase que cumpla con la interfaz definida en *Builder* pero que use internamente la librer√≠a de whoosh.\n","\n","La entrada para construir el √≠ndice (m√©todo Builder.build()) podr√° ser, tal y como se puede ver en el programa de prueba al final de este notebook, a) un fichero de texto con direcciones Web (una por l√≠nea); b) una carpeta del disco (se indexar√°n todos los ficheros de la carpeta, sin entrar en subcarpetas); o c) un archivo zip que contiene archivos comprimidos a indexar. Para simplificar, supondremos que el contenido a indexar es siempre HTML."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Eu23hSs6_wvX"},"outputs":[],"source":["import whoosh\n","from whoosh.fields import Schema, TEXT, ID\n","from whoosh.formats import Format\n","from whoosh.qparser import QueryParser\n","from zipfile import ZipFile\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import ssl\n","\n","\n","# A schema in Whoosh is the set of possible fields in a document in the search space. \n","# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n","# and a content.\n","Document = Schema(\n","        path=ID(stored=True),\n","        content=TEXT(vector=Format))\n","\n","class WhooshBuilder():\n","    \"\"\"\n","    Whoosh Builder:\n","    Class to build a Whoosh index from a collection of documents.\n","    The collection can be a directory of files, a zip file, or a text file with a list of URLs.\n","\n","    @param index_path : str\n","        Path to the index directory.\n","    @attribute index_path : str\n","        Path to the index directory.\n","    @attribute writer : whoosh.index.Writer\n","        Whoosh index writer.\n","    \"\"\"\n","    index_path = None\n","    writer = None\n","    \n","    def __init__(self, index_path):\n","        self.index_path = index_path\n","        \n","\n","    def build(self, collections):\n","        \"\"\"\n","        build(collections)\n","            - Build the index from a collection of documents.\n","            - The collection can be a directory of files, a zip file, or a text file with a list of URLs.\n","        @param collections: path to the collection.\n","        \"\"\"\n","        ctx = ssl.create_default_context()\n","        ctx.check_hostname = False\n","        ctx.verify_mode = ssl.CERT_NONE\n","        if os.path.exists(self.index_path): shutil.rmtree(self.index_path)\n","        os.makedirs(self.index_path)\n","\n","        self.writer = whoosh.index.create_in(self.index_path, Document).writer()\n","        if os.path.isdir(collections):\n","            for path in sorted(os.listdir(collections)):\n","                fn=os.path.join(collections, path)\n","                if os.path.isfile(fn):\n","                    with open(fn, 'r') as f:\n","                        self.writer.add_document(path=fn, content=f.read())\n","\n","        elif collections.endswith(\".txt\"):\n","            with open(collections) as f:\n","                    content = f.readlines()\n","                    for l in content:\n","                        l = l.strip()\n","                        self.writer.add_document(path=l, content=BeautifulSoup(urlopen(l, context=ctx).read(), \"html.parser\").text)\n","\n","        elif collections.endswith(\".zip\"):\n","            for l in ZipFile(collections, \"r\").namelist():\n","                    self.writer.add_document(path=l, content=BeautifulSoup(ZipFile(collections, \"r\").read(l), \"html.parser\").text)\n","        return\n","\n","    def commit(self):\n","        \"\"\"\n","        commit()\n","            - Commit the index.\n","        \"\"\"\n","        self.writer.commit()\n","\n","class WhooshIndex():\n","    \"\"\"\n","    Whoosh Index:\n","    Class to access a Whoosh index.\n","    \n","    @param dir : str\n","        Path to the index directory.\n","    @attribute index : whoosh.index.Index\n","        Whoosh index.\n","    @attribute reader : whoosh.index.Reader\n","        Whoosh index reader.\n","    @attribute path : str\n","        Path to the index directory.\n","    \"\"\"\n","    reader = None\n","    index = None\n","    path = \"\"\n","\n","    def __init__(self, dir) -> None:\n","        self.index = whoosh.index.open_dir(dir)\n","        self.reader = self.index.reader()\n","        self.path=dir\n","\n","    def ndocs(self):\n","        \"\"\"\n","        ndocs()\n","            - Return the number of documents in the index.\n","        \"\"\"\n","        return self.reader.doc_count()\n","\n","    def all_terms(self):\n","        \"\"\"\n","        all_terms()\n","            - Return a list of all terms in the index.\n","        \"\"\"\n","        return list(self.reader.field_terms(\"content\"))\n","\n","    def all_terms_with_freq(self):\n","        \"\"\"\n","        all_terms_with_freq()\n","            - Return a list of all terms in the index with their frequency.\n","        \"\"\"\n","        res=[]\n","        for i in self.all_terms():\n","            res.append((i, self.total_freq(i)))\n","        return res\n","\n","    def total_freq(self, term):\n","        \"\"\"\n","        total_freq(term)\n","            - Return the total frequency of a term in the index.\n","        @param term: term to search\n","        \"\"\"\n","        return self.reader.frequency(\"content\", term)\n","\n","    def doc_path(self, doc_id):\n","        \"\"\"\n","        doc_path(doc_id)\n","            - Return the path of a document given its id.\n","            @param doc_id: id of the document\n","        \"\"\"\n","        return self.reader.stored_fields(doc_id)['path']\n","\n","    def term_freq(self, term, doc_id):\n","        \"\"\"\n","        term_freq(term, doc_id)\n","            - Return the frequency of a term in a document given its id.\n","            @param term: term to search\n","            @param doc_id: id of the document\n","        \"\"\"\n","        for p in self.reader.postings(\"content\", term).items_as(\"frequency\") if self.reader.doc_frequency(\"content\", term) > 0 else []:\n","            if doc_id == p[0]:\n","                return p[1]\n","        return 0\n","\n","    def doc_freq(self, term):\n","        \"\"\"\n","        doc_freq(term)\n","            - Return the document frequency of a term in the index.\n","            @param term: term to search\n","        \"\"\"\n","        return self.reader.doc_frequency(\"content\", term)\n","\n","    def doc_terms_with_freq(self, doc_id):\n","        \"\"\"\n","        doc_terms_with_freq(doc_id)\n","            - Return a list of terms in a document given its id.\n","            @param doc_id: id of the document\n","        \"\"\"\n","        freqs = []\n","        for term in self.reader.vector_as(\"frequency\", doc_id, \"content\"):\n","            freqs.append((term[0], term[1]))\n","        return freqs\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JGjT-DbW-yHR"},"source":["### Explicaci√≥n/documentaci√≥n\n","\n","En este ejercicio se crean las clases 'WhooshBuilder' y 'WhooshIndex' las cuales implementan la funcionalidad de la creacion de un √≠ndice y la gesti√≥n del mismo respectivemente. Para afrontar este ejercicio nos hemos basado en el c√≥digo de ejemplo proporcionado al principio de este documento, y para saber que m√©todos se requer√≠an en cada una de las clases hemos ido siguiendo el c√≥digo de prueba al final del documento e identificado cada uno de los m√©todos que se necesitaban. A posteriori, tambi√©n hemos a√±adido algumna funci√≥n que nos pod√≠a ser √∫til a parte de las expl√≠citamente indicadas en el c√≥digo de prueba (como es el caso de doc_terms_with_freq en Index). \n","\n","\n","**WhooshBuilder**\n","\n","El constructor de esta clase simplemente guardaba el index_path para su posterior uso. La mayor parte de la funcionalidad de esta clase reside en el m√©todo build. El m√©todo build crea un √≠ndice de b√∫squeda utilizando la biblioteca Whoosh. Toma una lista de colecciones como argumento y luego lee cada archivo en la lista y agrega sus contenidos al √≠ndice. El √≠ndice se crea en la ruta especificada en la variable self.index_path.\n","\n","Si la lista de colecciones es un directorio, la funci√≥n recorre cada archivo en el directorio y agrega sus contenidos al √≠ndice. Si la lista de colecciones es un archivo de texto (.txt), la funci√≥n lee cada l√≠nea del archivo de texto y utiliza la biblioteca BeautifulSoup para extraer el contenido HTML de cada URL y agregarlo al √≠ndice. Si la lista de colecciones es un archivo ZIP (.zip), la funci√≥n lee cada archivo dentro del archivo ZIP y agrega sus contenidos al √≠ndice.\n","\n","Finalmente ncontramos el m√©todo commit el cual simplemente hace que los documentos agregados est√©n disponibles para la b√∫squeda. Esta funci√≥n confirma los cambios y escribe los documentos agregados en el √≠ndice en el disco. \n","\n","\n","**WhooshIndex**\n","\n","La clase WhooshIndex se utiliza para realizar diferentes operaciones de b√∫squeda y an√°lisis de texto sobre un √≠ndice Whoosh. Cada uno de los m√©todos es autodescriptivo por su nombre y simplemente sustituye operaciones b√°sicas sobre un Indice Whoosh. \n","\n","\n","Decidimos eliminar las clases abstractas Index y Builder ya que no tenian uso al estar definidas inmediatamente despues. \n"]},{"cell_type":"markdown","metadata":{"id":"YEOpKvZi9cos"},"source":["## Ejercicio 1.2: B√∫squeda (2pt)\n","\n","Implementar la clase WhooshSearcher como subclase de Searcher."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XrowpYaLmMxX"},"outputs":[],"source":["import math\n","import re\n","\n","def from_query_to_terms(text):\n","    return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IGtH0CpF9owH"},"outputs":[],"source":["class WhooshSearcher():\n","    \"\"\"\n","    Whoosh Searcher.\n","    Class to search a Whoosh index.\n","    \n","    @param index_path : str\n","        Path to the index directory.\n","    @attribute index : whoosh.index.Index\n","        Whoosh index.\n","    @attribute searcher : whoosh.searching.Searcher\n","        Whoosh searcher.\n","    @attribute qparser : whoosh.qparser.QueryParser\n","        Whoosh query parser.\n","    \"\"\"\n","    index = None\n","    searcher = None\n","    qparser = None\n","\n","    def __init__(self, index_path):\n","        self.index = whoosh.index.open_dir(index_path)\n","        self.searcher = self.index.searcher()\n","        self.qparser = QueryParser(\"content\", schema=self.index.schema)\n","\n","    def search(self, query, cutoff):\n","        \"\"\"\n","        search(query, cutoff)\n","            - Search the index for a query.\n","        @param query: query to search\n","        @param cutoff: number of results to return\n","        @return: list of tuples (path, score)\n","        \"\"\"\n","        result = []\n","        search_results = list(self.searcher.search(self.qparser.parse(query)).items())[:cutoff]\n","\n","        for docid, score in search_results:\n","            result.append((self.index.reader().stored_fields(docid)['path'], score))\n","        return result\n","        \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DNDHrfjN-zTV"},"source":["### Explicaci√≥n/documentaci√≥n\n","\n","La clase WhooshSearcher encapsula un buscador \"Searcher\" orientado a obtener resultados sobre el √≠ndice que obtiene como argumento.\n","Para cumplir su prop√≥sito hace uso de la librer√≠a whoosh, para abrir y leer el √≠ndice. Adem√°s del constructor, la clase WhooshSearcher\n","contiene un m√©todo search. √âste m√©todo devuelve los N documentos del √≠ndice que m√°s puntuaci√≥n obtienen, estando la puntuaci√≥n directamente\n","relacionada con las apariciones y frecuencia de los t√©rminos introducidos como \"query\" en el argumento 1, siendo N el argumento 2.\n","Ya que utilizamos las funciones implementadas con la librer√≠a whoosh, no podemos ver de manera directa el c√≥digo que se utiliza para obtener los\n","scores."]},{"cell_type":"markdown","metadata":{"id":"JH9u7bWi9pGc"},"source":["# Ejercicio 2: Modelo vectorial\n","\n","Implementar dos modelos de r√°nking propios, basados en el modelo vectorial."]},{"cell_type":"markdown","metadata":{"id":"SphKCsSN9udY"},"source":["## Ejercicio 2.1: Producto escalar (2.5pt)\n","\n","Implementar un modelo vectorial propio que utilice el producto escalar (sin dividir por las normas de los vectores) como funci√≥n de r√°nking, por medio de la clase VSMDotProductSearcher, como subclase de Searcher.\n","\n","Este modelo har√° uso de la clase Index y se podr√° probar con la implementaci√≥n WhooshIndex (puedes ver un ejemplo de esto en la celda de prueba).\n","\n","Adem√°s, la clase VSMDotProductSearcher ser√° intercambiable con WhooshSearcher, como se puede ver en la celda de prueba, donde la funci√≥n test_search utiliza una implementaci√≥n u otra sin distinci√≥n."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6JQ2xbth-A-G"},"outputs":[],"source":["from whoosh.searching import Searcher\n","\n","\n","\n","def tf(freq):\n","    \"\"\"\n","    tf(freq)\n","        - Return the term frequency.\n","    @param freq: term frequency\n","    \"\"\"\n","    return 1 + math.log2(freq) if freq > 0 else 0\n","\n","\n","def idf(df, n):\n","    \"\"\"\n","    idf(df, n)\n","        - Return the inverse document frequency.\n","    @param df: document frequency\n","    @param n: number of documents\n","    \"\"\"\n","    return math.log2((n + 1) / (df + 0.5))\n","\n","\n","\n","class VSMDotProductSearcher():\n","    \"\"\"\n","    VSM Dot Product Searcher\n","    Class to search a Whoosh index using the VSM model with the dot product.\n","\n","    @param index_path : str\n","        - Path to the index directory.\n","    \"\"\"\n","    def __init__(self, index):\n","        self.index = index\n","\n","    def search(self, query, cutoff):\n","        \"\"\"\n","        search(query, cutoff)\n","            - Search the index for a query.\n","        @param query: query to search\n","        @param cutoff: number of results to return\n","        @return: list of tuples (path, score)\n","        \"\"\"\n","        docs=[]\n","        terms = from_query_to_terms(query)\n","        for doc_id in range(self.index.ndocs()):\n","            score=0\n","            for term in terms:\n","                score += tf(self.index.term_freq(term, doc_id)) * idf(self.index.doc_freq(term), self.index.ndocs())\n","            if score > 0:\n","                docs.append([self.index.doc_path(doc_id), score])\n","        docs.sort(key=lambda x: x[1], reverse=True)\n","        return docs[0:cutoff]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UgumTPoT-1WD"},"source":["### Explicaci√≥n/documentaci√≥n\n","\n","La clase VSMDotProductSearcher es una subclase de Searcher que implementa un modelo vectorial de b√∫squeda utilizando el producto escalar sin dividir por las normas de los vectores. Esto se logra mediante el c√°lculo de la puntuaci√≥n de cada documento en funci√≥n de la frecuencia del t√©rmino en el documento y la frecuencia del t√©rmino en el √≠ndice. Para cada documento en el √≠ndice, se calcula su puntuaci√≥n sumando las puntuaciones de todos los t√©rminos en la consulta que aparecen en el documento. Si la puntuaci√≥n del documento es mayor que cero, se agrega a la lista de resultados.\n","\n","La funci√≥n tf calcula la frecuencia de t√©rmino ajustada (tf) para un t√©rmino en un documento. La funci√≥n idf calcula el factor de ponderaci√≥n inverso del documento (idf) para un t√©rmino en el √≠ndice. La suma de los valores tf y idf para cada t√©rmino se utiliza para calcular la puntuaci√≥n de un documento.\n","\n","La funci√≥n search toma una consulta y un l√≠mite superior de resultados, y devuelve una lista de los documentos m√°s relevantes ordenados por puntuaci√≥n decreciente. La consulta se divide en t√©rminos y se busca la puntuaci√≥n de cada documento en funci√≥n de los t√©rminos en la consulta. Los resultados se devuelven como una lista de pares de documentos y puntuaciones.\n","\n","Esta implementaci√≥n es justificable porque el producto escalar es una medida com√∫n de similitud en espacios vectoriales, y no dividir por las normas de los vectores puede simplificar los c√°lculos y no afectar significativamente la calidad de los resultados. Adem√°s, la implementaci√≥n utiliza las funciones tf e idf est√°ndar en el procesamiento de lenguaje natural, lo que mejora la precisi√≥n de los resultados. En general, la implementaci√≥n de VSMDotProductSearcher es una forma razonable y eficiente de realizar b√∫squedas en un √≠ndice de Whoosh."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8o5TvvE5-BVK"},"source":["### Ejercicio\n","\n","A√±adir a mano un documento a la colecci√≥n docs1k.zip de manera que aparezca el primero para la consulta ‚Äúobama family tree‚Äù para este buscador. Documentar c√≥mo se ha conseguido y por qu√© resulta as√≠.\n","\n","### Soluci√≥n\n","\n","Para obtener el score con producto escalar multiplicamos la frecuencia de los t√©rminos en el documento por la funci√≥n idf, siendo esta \"log2((n + 1) / (df + 0.5))\".\n","Para obtener un score lo m√°s grande posible, por lo tanto, hay dos relaciones que podemos modificar.\n","Por un lado podemos aumentar la frecuencia de cada t√©rmino (respecto del documento), que modifica directamente el valor del score.\n","Por otro lado, necesitamos que la funci√≥n idf devuelva el valor m√°s alto posible. Como el n√∫mero de documentos es constante, lo m√°s optimo es que (df + 0.5) sea\n","lo menor posible, siendo df la frecuencia total de los t√©rminos en b√∫squeda.\n","Por lo tanto, para obtener el mejor resultado posible hay que conseguir un balance entre la frecuencia de los t√©rminos por documento y la frecuencia de los t√©rminos en relaci√≥n al resto de documentos del √≠ndice. Como la funci√≥n idf escala de forma logar√≠tmica, y en cambio, la cantidad de apariciones de cada t√©rmino escala de forma lineal, hemos decidido utilizar un documento con la query utilizada (\"obama family tree\") repetida un gran n√∫mero de veces."]},{"cell_type":"markdown","metadata":{"id":"qPPNbWNe-HcR"},"source":["## Ejercicio 2.2: Coseno (2pt)\n","\n","Refinar la implementaci√≥n del modelo para que calcule el coseno, definiendo para ello una clase VSMCosineSearcher. Para ello se necesitar√° extender Builder (o WhooshBuilder) con el c√°lculo de los m√≥dulos de los vectores, que deber√°n almacenarse en un fichero, en la carpeta de √≠ndice junto a los ficheros que genera cada √≠ndice. \n","\n","Pensad en qu√© parte del dise√±o interesa hacer esto, en concreto, qu√© clase y en qu√© momento tendr√≠a que calcular, devolver y/o almacenar estos m√≥dulos."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9n30WQX_-RMR"},"outputs":[],"source":["class VSMCosineSearcher():\n","    \"\"\"\n","    VSMCosineSearcher\n","    Class to search a Whoosh index using the VSM with cosine similarity.\n","    \n","    @param index : WhooshSearcher\n","        - Whoosh index\n","    @attribute index : WhooshSearcher\n","        - Whoosh index\n","    @attribute modules : list\n","        - List of modules of the documents in the index\n","    \"\"\"\n","    index = None\n","    modules = []\n","\n","    def __init__(self, index):\n","        self.index = index\n","        self.modules = []\n","        for doc_id in range(index.ndocs()):\n","            div, divd, divq = 0, 0, 0\n","            for vec in index.doc_terms_with_freq(doc_id):\n","                divd = (tf(vec[1])) ** 2 \n","                divq = (idf(index.doc_freq(vec[0]), index.ndocs())) ** 2\n","                div += divd*divq\n","            div=math.sqrt(div)\n","            self.modules.append(div)\n","\n","\n","\n","\n","    def search(self, query, cutoff):\n","        \"\"\"\n","        search(query, cutoff)\n","            - Search the index for a query.\n","        @param query: query to search\n","        @param cutoff: number of results to return\n","        @return: list of tuples (path, score)\n","        \"\"\"\n","        docs=[]\n","        terms = from_query_to_terms(query)\n","        for doc_id in range(self.index.ndocs()):\n","            score=0\n","            for term in terms:\n","                score += ((tf(self.index.term_freq(term, doc_id)) * idf(self.index.doc_freq(term), self.index.ndocs())) / self.modules[doc_id])\n","            if score > 0:\n","                docs.append([self.index.doc_path(doc_id), score])\n","        docs.sort(key=lambda x: x[1], reverse=True)\n","        return docs[0:cutoff]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UJqVKt6o-2oB"},"source":["### Explicaci√≥n/documentaci√≥n\n","\n","En esta nueva implementaci√≥n, se ha refinado el modelo vectorial anterior para que calcule el coseno en lugar del producto escalar como funci√≥n de ranking, lo que puede proporcionar resultados m√°s precisos. Para ello, se ha creado la clase VSMCosineSearcher, que extiende la clase Searcher para permitir la b√∫squeda en el √≠ndice.\n","\n","La parte m√°s importante de esta nueva implementaci√≥n es el c√°lculo de los m√≥dulos de los vectores. Los m√≥dulos se calculan en el constructor de VSMCosineSearcher y se almacenan en una lista llamada modules. Para calcular el m√≥dulo de un vector, se utiliza la f√≥rmula:\n","\n","\n","cos = sum(tf(w) * idf(w))/ sqrt(sum(tf(w)^2 * idf(w)^2))\n","donde ùë°ùëì mide la ‚Äúimportancia‚Äù de los t√©rminos en los documentos e ùëñùëëùëì mide el poder de discriminaci√≥n del t√©rmino\n","\n","Al inicializar el buscador guardamos la parte de abajo del algortimo, sqrt(sum(tf(w)^2 * idf(w)^2)). Esto nos ayuda a tener parte de la funci√≥n ya realizada a la hora de satisfacer cada b√∫squeda.\n","\n","Cada vez que se utiliza el m√©todo \"search\" de la clase, ejecuta el resto de la f√≥rmula, sumando el score de los t√©rminos dependientes de la query\n","\n","Finalmente, se ordenan los resultados por score en orden descendente y se devuelve un m√°ximo de cutoff resultados.\n","\n","En cuanto al almacenamiento de los m√≥dulos, se ha decidido que se almacenen en un fichero en la carpeta del √≠ndice, junto con los ficheros que genera cada √≠ndice. Para ello, se debe extender la clase Builder o WhooshBuilder para que calcule y almacene los m√≥dulos en el momento en que se crea el √≠ndice."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wh83cdIu-Re1"},"source":["### Ejercicio\n","\n","A√±adir a mano un documento a la colecci√≥n docs1k.zip de manera que aparezca el primero para la consulta ‚Äúobama family tree‚Äù para este buscador. Documentar c√≥mo se ha conseguido y por qu√© resulta as√≠.\n","\n","### Soluci√≥n\n","\n","Para obtener el mismo resultado para la b√∫squeda de \"obama family tree\" con este nuevo CosineSearcher, la implementaci√≥n que debemos seguir es similar a la utilizada para el modelo de producto escalar, ya que a pesar de que la f√≥rmula que utilizamos para obtener el score varien, las dependencias se mantienen, estando relacionado directamente con la frecuencia de los elementos de la query por documento. Por ello, igualmente, hemos utilizado un nuevo documento llenandolo de una gran repetici√≥n de los t√©rminos de la b√∫squeda para obtener el mayor score posible."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3Wf1RFu8-V8P"},"source":["# Ejercicio 3: Estad√≠sticas de frecuencias (1pt)\n","\n","Utilizando las funcionalidades de la clase Index, implementar una funci√≥n term_stats que calcule a) las frecuencias totales en la colecci√≥n de los t√©rminos, ordenadas de mayor a menor, y b) el n√∫mero de documentos que contiene cada t√©rmino, igualmente de mayor a menor. Visualizar las estad√≠sticas obtenidas en dos gr√°ficas en escala log-log (dos gr√°ficas por cada colecci√≥n, seis gr√°ficas en total), que se mostrar√°n en el cuaderno entregado.\n","\n","De esta forma, podr√°s comprobar si las estad√≠sticas de la colecci√≥n siguen alg√∫n tipo de comportamiento esperado (como la conocida [Ley de Zipf](https://es.wikipedia.org/wiki/Ley_de_Zipf))."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def get_file_name(path):\n","    dir_name, file_name = os.path.split(path)\n","\n","    match = re.match(r'^(.*)\\.(.*?)$', file_name)\n","    if match:\n","        return match.group(1)\n","    else:\n","        return file_name\n","\n","def term_stats(index):\n","    # las frecuencias totales en la colecci√≥n de los t√©rminos, ordenadas de mayor a menor,\n","    freqs = sorted(index.all_terms_with_freq(), key=lambda x: x[1], reverse=True)\n","    # Visualizar las estad√≠sticas obtenidas en dos gr√°ficas en escala log-log\n","    plt.loglog([x[1] for x in freqs])\n","    plt.xlabel(\"Term\")\n","    plt.ylabel(\"Frequency\")\n","    plt.title(\"Total term frequency on : \"+get_file_name(index.path))\n","    plt.grid(True)\n","    plt.savefig(\"term_frequency_\"+get_file_name(index.path)+\".png\")\n","    plt.clf()\n","    # el n√∫mero de documentos que contiene cada t√©rmino, igualmente de mayor a menor.\n","    docs_per_term = sorted([(term, index.doc_freq(term)) for term in index.all_terms()], key=lambda x: x[1], reverse=True)\n","    plt.loglog([x[1] for x in docs_per_term])\n","    plt.xlabel(\"Term\")\n","    plt.ylabel(\"Documents\")\n","    plt.title(\"Number of documents per term on : \"+get_file_name(index.path))\n","    plt.grid(True)\n","    plt.savefig(\"documents_per_term_\"+get_file_name(index.path)+\".png\")\n","    plt.clf()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SlXLFNH3-4MH"},"source":["### Explicaci√≥n/documentaci√≥n\n","Para obtener los diferentes plots, en el c√≥digo al final del documento hemos a√±adido una linea justo despu√©s de la creaci√≥n del Indice que llamaba a esta funcion. Esta funci√≥n simplemente genera dos tipos de gr√°ficos para cada una de las colecciones. Una gr√°fica muestra el numero de documentos en los que aparece cada t√©rmino ordenado de mayor numero de apariciones a menor (documents_per_term_XXX.png) y la otra gr√°fica muestra  la frecuencia de cada uno de los t√©rminos enla colecci√≥n. \n","\n","Es importante destacar que cuanto mayor numero de datos se recogen mas se parece la grafica resultante a la ley de Zipf la cual sigue la siguiente funci√≥n P~1/n¬™.\n","\n","\n","### Im√°genes de los plots:\n","![Alt text](plots/documents_per_term_toy.png)\n","![Alt text](plots/term_frequency_toy.png)\n","![Alt text](plots/documents_per_term_urls.png)\n","![Alt text](plots/term_frequency_urls.png)\n","![Alt text](plots/documents_per_term_docs.png)\n","![Alt text](plots/term_frequency_docs.png)\n"]},{"cell_type":"markdown","metadata":{"id":"UfgNDMM6-e7k"},"source":["# Celda de prueba\n","\n","Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta *collections* en el mismo directorio que este *notebook*. El fichero *toy.zip* hay que descomprimirlo para indexar la carpeta que contiene."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"VXS8648MzPO3","outputId":"f0523523-9462-499e-ba4a-b6be2679718a"},"outputs":[{"name":"stdout","output_type":"stream","text":["=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Done ( 0.026472091674804688 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 4\n","Vocabulary size: 39\n","  Top 5 most frequent terms:\n","\taa\t9.0=9.0\n","\tbb\t5.0=5.0\n","\tsleep\t5.0=5.0\n","\tcc\t3.0=3.0\n","\tdie\t2.0=2.0\n","\n","\n","  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n","  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n","  Docs containing the word'cc': 2\n","Done ( 0.0013790130615234375 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'aa dd'\n","\n","Done ( 0.0009815692901611328 seconds )\n","\n","  VSMDotProductSearcher for query 'aa dd'\n","4.0 \t ./collections/toy/d1.txt\n","1.7369655941662063 \t ./collections/toy/d2.txt\n","1.0 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0009028911590576172 seconds )\n","\n","  VSMCosineSearcher for query 'aa dd'\n","1.0 \t ./collections/toy/d2.txt\n","0.7427813527082074 \t ./collections/toy/d1.txt\n","0.5773502691896258 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0014188289642333984 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/urls.txt\n","Done ( 3.5340583324432373 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 3\n","Vocabulary size: 6058\n","  Top 5 most frequent terms:\n","\tentropy\t375.0=375.0\n","\tbias\t201.0=201.0\n","\tsystem\t154.0=154.0\n","\tdisplaystyle\t138.0=138.0\n","\theat\t111.0=111.0\n","\n","\n","  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n","  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n","  Docs containing the word'wikipedia': 3\n","Done ( 0.14136505126953125 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'information probability'\n","2.9256657209314 \t https://en.wikipedia.org/wiki/Entropy\n","2.762888436480296 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","2.0995062376814877 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.00875091552734375 seconds )\n","\n","  VSMDotProductSearcher for query 'information probability'\n","2.1879764911644646 \t https://en.wikipedia.org/wiki/Entropy\n","1.301295829346397 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","1.155870467654375 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.0006482601165771484 seconds )\n","\n","  VSMCosineSearcher for query 'information probability'\n","0.024351038531673412 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","0.01743041087762228 \t https://en.wikipedia.org/wiki/Entropy\n","0.00959731136934486 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.0007748603820800781 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/docs1k.zip\n","Done ( 103.94468092918396 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 1.807939052581787 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.2532188892364502 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 4.182596921920776 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 2.493206024169922 seconds )\n","\n","=================================================================\n","Testing indices and search on 3 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Collection: ./collections/urls.txt\n","Collection: ./collections/docs1k.zip\n","Done ( 119.31910181045532 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 1.9052791595458984 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.17962408065795898 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 2.6829638481140137 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 3.0209431648254395 seconds )\n","\n"]}],"source":["import os\n","import shutil\n","import time\n","\n","def clear (index_path: str):\n","    if os.path.exists(index_path): shutil.rmtree(index_path)\n","    else: print(\"Creating \" + index_path)\n","    os.makedirs(index_path)\n","\n","def test_collection(collection_paths: list, index_path: str, word: str, query: str):\n","    start_time = time.time()\n","    print(\"=================================================================\")\n","    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n","\n","    # Let's create the folder if it did not exist\n","    # and delete the index if it did\n","    clear(index_path)\n","\n","    # We now test building an index\n","    test_build(WhooshBuilder(index_path), collection_paths)\n","\n","    # We now inspect the index\n","    index = WhooshIndex(index_path)\n","    test_read(index, word)\n","\n","    print(\"------------------------------\")\n","    print(\"Checking search results\")\n","    test_search(WhooshSearcher(index_path), query, 5)\n","    test_search(VSMDotProductSearcher(WhooshIndex(index_path)), query, 5)\n","    test_search(VSMCosineSearcher(WhooshIndex(index_path)), query, 5)\n","\n","def test_build(builder, collections: list):\n","    stamp = time.time()\n","    print(\"Building index with\", type(builder))\n","    for collection in collections:\n","        print(\"Collection:\", collection)\n","        # This function should index the received collection and add it to the index\n","        builder.build(collection)\n","    # When we commit, the information in the index becomes persistent\n","    # we can also save any extra information we may need\n","    # (and that cannot be computed until the entire collection is scanned/indexed)\n","    builder.commit()\n","    print(\"Done (\", time.time() - stamp, \"seconds )\")\n","    print()\n","\n","def test_read(index, word):\n","    stamp = time.time()\n","    print(\"Reading index with\", type(index))\n","    print(\"Collection size:\", index.ndocs())\n","    print(\"Vocabulary size:\", len(index.all_terms()))\n","    terms = index.all_terms_with_freq()\n","    terms.sort(key=lambda tup: tup[1], reverse=True)\n","    print(\"  Top 5 most frequent terms:\")\n","    for term in terms[0:5]:\n","        print(\"\\t\" + term[0] + \"\\t\" + str(term[1]) + \"=\" + str(index.total_freq(term)))\n","    print()\n","    # More tests\n","    doc_id = 0\n","    print()\n","    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n","    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n","    print(\"  Docs containing the word'\" + word + \"':\", index.doc_freq(word))\n","    print(\"Done (\", time.time() - stamp, \"seconds )\")\n","    print()\n","\n","\n","def test_search (engine, query, cutoff):\n","    stamp = time.time()\n","    print(\"  \" + engine.__class__.__name__ + \" for query '\" + query + \"'\")\n","    for path, score in engine.search(query, cutoff):\n","        print(score, \"\\t\", path)\n","    print()\n","    print(\"Done (\", time.time() - stamp, \"seconds )\")\n","    print()\n","\n","\n","index_root_dir = \"./index/\"\n","collections_root_dir = \"./collections/\"\n","test_collection ([collections_root_dir + \"toy/\"], index_root_dir + \"toy\", \"cc\", \"aa dd\")\n","test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls\", \"wikipedia\", \"information probability\")\n","test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n","test_collection ([collections_root_dir + \"toy/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"all_together\", \"seat\", \"obama family tree\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gTtUTF7j_QdF"},"source":["### Salida obtenida por el estudiante"]},{"cell_type":"raw","metadata":{},"source":["=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Done ( 0.02629995346069336 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 4\n","Vocabulary size: 39\n","  Top 5 most frequent terms:\n","\taa\t9.0=9.0\n","\tbb\t5.0=5.0\n","\tsleep\t5.0=5.0\n","\tcc\t3.0=3.0\n","\tdie\t2.0=2.0\n","\n","\n","  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n","  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n","  Docs containing the word'cc': 2\n","Done ( 0.003924846649169922 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'aa dd'\n","\n","Done ( 0.0007581710815429688 seconds )\n","\n","  VSMDotProductSearcher for query 'aa dd'\n","4.0 \t ./collections/toy/d1.txt\n","1.7369655941662063 \t ./collections/toy/d2.txt\n","1.0 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0008211135864257812 seconds )\n","\n","  VSMCosineSearcher for query 'aa dd'\n","1.0 \t ./collections/toy/d2.txt\n","0.7427813527082074 \t ./collections/toy/d1.txt\n","0.5773502691896258 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0008189678192138672 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/urls.txt\n","Done ( 3.2534189224243164 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 3\n","Vocabulary size: 6058\n","  Top 5 most frequent terms:\n","\tentropy\t375.0=375.0\n","\tbias\t201.0=201.0\n","\tsystem\t154.0=154.0\n","\tdisplaystyle\t138.0=138.0\n","\theat\t111.0=111.0\n","\n","\n","  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n","  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n","  Docs containing the word'wikipedia': 3\n","Done ( 0.18395709991455078 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'information probability'\n","2.9256657209314 \t https://en.wikipedia.org/wiki/Entropy\n","2.762888436480296 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","2.0995062376814877 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.007986068725585938 seconds )\n","\n","  VSMDotProductSearcher for query 'information probability'\n","2.1879764911644646 \t https://en.wikipedia.org/wiki/Entropy\n","1.301295829346397 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","1.155870467654375 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.001895904541015625 seconds )\n","\n","  VSMCosineSearcher for query 'information probability'\n","0.024351038531673412 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","0.01743041087762228 \t https://en.wikipedia.org/wiki/Entropy\n","0.00959731136934486 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.0006861686706542969 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/docs1k.zip\n","Done ( 143.3548059463501 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 3.8520541191101074 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.5448360443115234 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 4.355900764465332 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 3.4219231605529785 seconds )\n","\n","=================================================================\n","Testing indices and search on 3 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Collection: ./collections/urls.txt\n","Collection: ./collections/docs1k.zip\n","Done ( 128.99540185928345 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 2.325594186782837 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.1670520305633545 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 3.850494146347046 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 5.627086162567139 seconds )\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
