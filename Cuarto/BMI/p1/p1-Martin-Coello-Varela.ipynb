{"cells":[{"cell_type":"markdown","metadata":{"id":"PMuz6soJxEmP"},"source":["### **Búsqueda y Minería de Información 2022-23**\n","### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n","### Grado en Ingeniería Informática, 4º curso\n","# **Implementación de un motor de búsqueda**\n","\n","Fechas:\n","\n","* Comienzo: martes 7 / jueves 9 de febrero\n","* Entrega: martes 21 / jueves 23 de febrero (14:00)"]},{"cell_type":"markdown","metadata":{"id":"t-bdu5nO581M"},"source":["# Introducción"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5T91gVNMmMxO"},"source":["## Autores\n","\n","Guillermo Martín-Coello Juarez & Daniel Varela Sanchez"]},{"cell_type":"markdown","metadata":{"id":"xlYtzLZp6KwJ"},"source":["## Objetivos\n","\n","Los objetivos de esta práctica son:\n","\n","* La iniciación a la implementación de un motor de búsqueda.\n","*\tUna primera comprensión de los elementos básicos necesarios para implementar un motor completo.\n","*\tLa iniciación al uso de la librería [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) en Python para la creación y utilización de índices, funcionalidades de búsqueda en texto.\n","*\tLa iniciación a la implementación de una función de ránking sencilla.\n","\n","Los documentos que se indexarán en esta práctica, y sobre los que se realizarán consultas de búsqueda serán documentos HTML, que deberán ser tratados para extraer y procesar el texto contenido en ellos. \n","\n","La práctica plantea como punto de partida una pequeña API general sencilla (y cuyo uso se puede ver en un programa de prueba que se encuentra al final del enunciado), que pueda implementarse de diferentes maneras, como así se hará en esta práctica y las siguientes. A modo de toma de contacto y arranque de la asignatura, en esta primera práctica se completará una implementación de la API utilizando Whoosh, con lo que resultará bastante trivial la solución (en cuanto a la cantidad de código a escribir). En la siguiente práctica el estudiante desarrollará sus propias implementaciones, sustituyendo el uso de Whoosh que vamos a hacer en esta primera práctica.\n","\n","En términos de operaciones propias de un motor de búsqueda, en esta práctica el estudiante se encargará fundamentalmente de:\n","\n","a) En el proceso de indexación: recorrer los documentos de texto de una colección dada, eliminar del contenido posibles marcas tales como html, y enviar el texto a indexar por parte de Whoosh. \n","\n","b) En el proceso de responder consultas: implementar una primera versión sencilla de una o dos funciones de ránking en el modelo vectorial, junto con alguna pequeña estructura auxiliar."]},{"cell_type":"markdown","metadata":{"id":"_AjDofIw6Ns6"},"source":["## Material proporcionado\n","\n","Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n","\n","*\tVarias clases e interfaces Python (mayormente incompletas) a lo largo de este *notebook*, desde las que el estudiante partirá para completar código e integrará con ellas las suyas propias. \n","La celda de prueba *al final de este notebook* implementa un programa que deberá funcionar con el código a implementar por el estudiante. Además, se proporciona a continuación una celda con código ejemplo que ilustra las funciones más útiles de la API de Whoosh.\n","*\tUna pequeña colección <ins>docs1k.zip</ins> con aproximadamente 1.000 documentos HTML, y un pequeño fichero <ins>urls.txt</ins>. Ambas representan colecciones de prueba para depurar las implementaciones y comprobar su corrección.\n","*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la celda de prueba (salvo los tiempos de ejecución que pueden cambiar, aunque la tendencia en cuanto a qué métodos tardan más o menos debería cumplirse)."]},{"cell_type":"markdown","metadata":{"id":"q9udSskn_eY7"},"source":["## Ejemplo API Whoosh\n","\n","En la siguiente celda de código se incluyen varios ejemplos para comprobar cómo usar la API de la librería *Whoosh*."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"iTYZA7PYmMxR","outputId":"525f276d-fc44-45a4-8989-c6424209861a"},"outputs":[{"ename":"URLError","evalue":"<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)>","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[1;32m   1349\u001b[0m               encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m   1350\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1454\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m-> 1454\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mwrap_socket(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msock,\n\u001b[1;32m   1455\u001b[0m                                       server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:512\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    507\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    510\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    513\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    514\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    515\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    516\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    517\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    518\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    519\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    520\u001b[0m     )\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1070\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1070\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1071\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1341\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1341\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1342\u001b[0m \u001b[39mfinally\u001b[39;00m:\n","\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 57\u001b[0m\n\u001b[1;32m     51\u001b[0m urls \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Simpson\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms_paradox\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Bias\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Entropy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mindex/whoosh/example/urls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m whooshexample_buildindex(\u001b[39mdir\u001b[39;49m, urls)\n\u001b[1;32m     58\u001b[0m whooshexample_search(\u001b[39mdir\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m whooshexample_examine(\u001b[39mdir\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprobability\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mwhooshexample_buildindex\u001b[0;34m(dir, urls)\u001b[0m\n\u001b[1;32m     18\u001b[0m writer \u001b[39m=\u001b[39m whoosh\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mcreate_in(\u001b[39mdir\u001b[39m, Document)\u001b[39m.\u001b[39mwriter()\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m urls:\n\u001b[0;32m---> 20\u001b[0m     writer\u001b[39m.\u001b[39madd_document(path\u001b[39m=\u001b[39murl, content\u001b[39m=\u001b[39mBeautifulSoup(urlopen(url)\u001b[39m.\u001b[39mread(), \u001b[39m\"\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     21\u001b[0m writer\u001b[39m.\u001b[39mcommit()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[1;32m   1392\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[39mexcept\u001b[39;00m:\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:997)>"]}],"source":["# Whoosh API\n","import whoosh\n","from whoosh.fields import Schema, TEXT, ID\n","from whoosh.formats import Format\n","from whoosh.qparser import QueryParser\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import os, os.path\n","import shutil\n","\n","Document = Schema(\n","        path=ID(stored=True),\n","        content=TEXT(vector=Format))\n","\n","def whooshexample_buildindex(dir, urls):\n","    if os.path.exists(dir): shutil.rmtree(dir)\n","    os.makedirs(dir)\n","    writer = whoosh.index.create_in(dir, Document).writer()\n","    for url in urls:\n","        writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n","    writer.commit()\n","\n","def whooshexample_search(dir, query):\n","    index = whoosh.index.open_dir(dir)\n","    searcher = index.searcher()\n","    qparser = QueryParser(\"content\", schema=index.schema)\n","    print(\"Search results for '\", query, \"'\")\n","    for docid, score in searcher.search(qparser.parse(query)).items():\n","        print(score, \"\\t\", index.reader().stored_fields(docid)['path'])\n","    print()\n","\n","def whooshexample_examine(dir, term, docid, n):\n","    reader = whoosh.index.open_dir(dir).reader()\n","    print(\"Total nr. of documents in the collection:\", reader.doc_count())\n","    print(\"Total frequency of '\", term, \"':\", reader.frequency(\"content\", term))\n","    print(\"Nr. documents containing '\", term, \"':\", reader.doc_frequency(\"content\", term))\n","    for p in reader.postings(\"content\", term).items_as(\"frequency\") if reader.doc_frequency(\"content\", term) > 0 else []:\n","        print(\"\\tFrequency of '\", term, \"' in document\", p[0], \":\", p[1])\n","    raw_vec = reader.vector(docid, \"content\")\n","    raw_vec.skip_to(term)\n","    if raw_vec.id() == term:\n","        print(\"Frequency of '\", raw_vec.id(), \"' in document\", docid, reader.stored_fields(docid)['path'], \":\", raw_vec.value_as(\"frequency\"))\n","    else:\n","        print(\"Term '\", term, \"' not found in document\", docid)\n","    print(\"Top\", n, \"most frequent terms in document\", docid, reader.stored_fields(docid)['path']) \n","    vec = reader.vector(docid, \"content\").items_as(\"frequency\")\n","    for p in sorted(vec, key=lambda x: x[1], reverse=True)[0:n]:\n","        print(\"\\t\", p)\n","    print()\n","\n","urls = [\"https://en.wikipedia.org/wiki/Simpson's_paradox\", \n","        \"https://en.wikipedia.org/wiki/Bias\",\n","        \"https://en.wikipedia.org/wiki/Entropy\"]\n","\n","dir = \"index/whoosh/example/urls\"\n","\n","whooshexample_buildindex(dir, urls)\n","whooshexample_search(dir, \"probability\")\n","whooshexample_examine(dir, \"probability\", 0, 5)"]},{"cell_type":"markdown","metadata":{"id":"2ciKzD4D6Xn6"},"source":["## Calificación\n","\n","Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado.  \n","\n","El peso de la nota de esta práctica en la calificación final de prácticas es del **20%**.\n","\n","La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. \n","La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n","\n","Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) **sin ninguna modificación**. El profesor comprobará este aspecto ejecutando la celda de prueba así como otras pruebas adicionales."]},{"cell_type":"markdown","metadata":{"id":"nnln3zQV6anE"},"source":["## Entrega\n","\n","La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**. Si se necesita entregar algún fichero adicional (por ejemplo, imágenes) se puede subir un fichero ZIP a la tarea correspondiente de Moodle. En cualquiera de los dos casos, el nombre del fichero a subir será **bmi-p1-XX**, donde XX debe sustituirse por el número de pareja (01, 02, ..., 10, ...).\n","\n","En concreto, se debe documentar:\n","\n","- Qué version(es) del modelo vectorial se ha(n) implementado en el ejercicio 2.\n","- Cómo se ha conseguido colocar un documento en la primera posición de ránking, para cada buscador implementado en el ejercicio 2.\n","- El trabajo realizado en el ejercicio 3. \n","- Y cualquier otro aspecto que el estudiante considere oportuno destacar.\n"]},{"cell_type":"markdown","metadata":{"id":"6GtUWMA76bP0"},"source":["## Indicaciones\n","\n","Se podrán definir clases adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: \n","\n","*\tNo deberá editarse el código proporcionado más allá de donde se indica explícitamente.\n","*\t**La celda de prueba deberá ejecutar** correctamente sin ninguna modificación."]},{"cell_type":"markdown","metadata":{"id":"gICjfJ-B6g0Y"},"source":["# Ejercicio 1: Implementación basada en Whoosh\n","\n","Implementar las clases y módulos necesarios para que la celda de prueba funcione. Se deja al estudiante deducir alguna de las relaciones jerárquicas entre las clases Python."]},{"cell_type":"markdown","metadata":{"id":"m18qmDwAzANn"},"source":["## Ejercicio 1.1: Indexación (3.5pt)\n","\n","Definir las siguientes clases:\n","\n","* Index: clase general (no depende de Whoosh) y que encapsule los métodos necesarios para que funcione la celda de prueba que se encuentra al final del enunciado.\n","* Builder: clase general (no depende de Whoosh) que permite construir un índice (a través del método Builder.build()), tal y como se llama desde la celda de prueba entregada.\n","* WhooshIndex: clase que cumpla con la interfaz definida en *Index* usando la librería de whoosh.\n","* WhooshBuilder: clase que cumpla con la interfaz definida en *Builder* pero que use internamente la librería de whoosh.\n","\n","La entrada para construir el índice (método Builder.build()) podrá ser, tal y como se puede ver en el programa de prueba al final de este notebook, a) un fichero de texto con direcciones Web (una por línea); b) una carpeta del disco (se indexarán todos los ficheros de la carpeta, sin entrar en subcarpetas); o c) un archivo zip que contiene archivos comprimidos a indexar. Para simplificar, supondremos que el contenido a indexar es siempre HTML."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Eu23hSs6_wvX"},"outputs":[],"source":["import whoosh\n","from whoosh.fields import Schema, TEXT, ID\n","from whoosh.formats import Format\n","from whoosh.qparser import QueryParser\n","from zipfile import ZipFile\n","from urllib.request import urlopen\n","from bs4 import BeautifulSoup\n","import ssl\n","\n","\n","# A schema in Whoosh is the set of possible fields in a document in the search space. \n","# We just define a simple 'Document' schema, with a path (a URL or local pathname)\n","# and a content.\n","Document = Schema(\n","        path=ID(stored=True),\n","        content=TEXT(vector=Format))\n","\n","class WhooshBuilder():\n","    \"\"\"\n","    Whoosh Builder:\n","    Class to build a Whoosh index from a collection of documents.\n","    The collection can be a directory of files, a zip file, or a text file with a list of URLs.\n","\n","    @param index_path : str\n","        Path to the index directory.\n","    @attribute index_path : str\n","        Path to the index directory.\n","    @attribute writer : whoosh.index.Writer\n","        Whoosh index writer.\n","    \"\"\"\n","    index_path = None\n","    writer = None\n","    \n","    def __init__(self, index_path):\n","        self.index_path = index_path\n","        \n","\n","    def build(self, collections):\n","        \"\"\"\n","        build(collections)\n","            - Build the index from a collection of documents.\n","            - The collection can be a directory of files, a zip file, or a text file with a list of URLs.\n","        @param collections: path to the collection.\n","        \"\"\"\n","        ctx = ssl.create_default_context()\n","        ctx.check_hostname = False\n","        ctx.verify_mode = ssl.CERT_NONE\n","        if os.path.exists(self.index_path): shutil.rmtree(self.index_path)\n","        os.makedirs(self.index_path)\n","\n","        self.writer = whoosh.index.create_in(self.index_path, Document).writer()\n","        if os.path.isdir(collections):\n","            for path in sorted(os.listdir(collections)):\n","                fn=os.path.join(collections, path)\n","                if os.path.isfile(fn):\n","                    with open(fn, 'r') as f:\n","                        self.writer.add_document(path=fn, content=f.read())\n","\n","        elif collections.endswith(\".txt\"):\n","            with open(collections) as f:\n","                    content = f.readlines()\n","                    for l in content:\n","                        l = l.strip()\n","                        self.writer.add_document(path=l, content=BeautifulSoup(urlopen(l, context=ctx).read(), \"html.parser\").text)\n","\n","        elif collections.endswith(\".zip\"):\n","            for l in ZipFile(collections, \"r\").namelist():\n","                    self.writer.add_document(path=l, content=BeautifulSoup(ZipFile(collections, \"r\").read(l), \"html.parser\").text)\n","        return\n","\n","    def commit(self):\n","        \"\"\"\n","        commit()\n","            - Commit the index.\n","        \"\"\"\n","        self.writer.commit()\n","\n","class WhooshIndex():\n","    \"\"\"\n","    Whoosh Index:\n","    Class to access a Whoosh index.\n","    \n","    @param dir : str\n","        Path to the index directory.\n","    @attribute index : whoosh.index.Index\n","        Whoosh index.\n","    @attribute reader : whoosh.index.Reader\n","        Whoosh index reader.\n","    @attribute path : str\n","        Path to the index directory.\n","    \"\"\"\n","    reader = None\n","    index = None\n","    path = \"\"\n","\n","    def __init__(self, dir) -> None:\n","        self.index = whoosh.index.open_dir(dir)\n","        self.reader = self.index.reader()\n","        self.path=dir\n","\n","    def ndocs(self):\n","        \"\"\"\n","        ndocs()\n","            - Return the number of documents in the index.\n","        \"\"\"\n","        return self.reader.doc_count()\n","\n","    def all_terms(self):\n","        \"\"\"\n","        all_terms()\n","            - Return a list of all terms in the index.\n","        \"\"\"\n","        return list(self.reader.field_terms(\"content\"))\n","\n","    def all_terms_with_freq(self):\n","        \"\"\"\n","        all_terms_with_freq()\n","            - Return a list of all terms in the index with their frequency.\n","        \"\"\"\n","        res=[]\n","        for i in self.all_terms():\n","            res.append((i, self.total_freq(i)))\n","        return res\n","\n","    def total_freq(self, term):\n","        \"\"\"\n","        total_freq(term)\n","            - Return the total frequency of a term in the index.\n","        @param term: term to search\n","        \"\"\"\n","        return self.reader.frequency(\"content\", term)\n","\n","    def doc_path(self, doc_id):\n","        \"\"\"\n","        doc_path(doc_id)\n","            - Return the path of a document given its id.\n","            @param doc_id: id of the document\n","        \"\"\"\n","        return self.reader.stored_fields(doc_id)['path']\n","\n","    def term_freq(self, term, doc_id):\n","        \"\"\"\n","        term_freq(term, doc_id)\n","            - Return the frequency of a term in a document given its id.\n","            @param term: term to search\n","            @param doc_id: id of the document\n","        \"\"\"\n","        for p in self.reader.postings(\"content\", term).items_as(\"frequency\") if self.reader.doc_frequency(\"content\", term) > 0 else []:\n","            if doc_id == p[0]:\n","                return p[1]\n","        return 0\n","\n","    def doc_freq(self, term):\n","        \"\"\"\n","        doc_freq(term)\n","            - Return the document frequency of a term in the index.\n","            @param term: term to search\n","        \"\"\"\n","        return self.reader.doc_frequency(\"content\", term)\n","\n","    def doc_terms_with_freq(self, doc_id):\n","        \"\"\"\n","        doc_terms_with_freq(doc_id)\n","            - Return a list of terms in a document given its id.\n","            @param doc_id: id of the document\n","        \"\"\"\n","        freqs = []\n","        for term in self.reader.vector_as(\"frequency\", doc_id, \"content\"):\n","            freqs.append((term[0], term[1]))\n","        return freqs\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JGjT-DbW-yHR"},"source":["### Explicación/documentación\n","\n","En este ejercicio se crean las clases 'WhooshBuilder' y 'WhooshIndex' las cuales implementan la funcionalidad de la creacion de un índice y la gestión del mismo respectivemente. Para afrontar este ejercicio nos hemos basado en el código de ejemplo proporcionado al principio de este documento, y para saber que métodos se requerían en cada una de las clases hemos ido siguiendo el código de prueba al final del documento e identificado cada uno de los métodos que se necesitaban. A posteriori, también hemos añadido algumna función que nos podía ser útil a parte de las explícitamente indicadas en el código de prueba (como es el caso de doc_terms_with_freq en Index). \n","\n","\n","**WhooshBuilder**\n","\n","El constructor de esta clase simplemente guardaba el index_path para su posterior uso. La mayor parte de la funcionalidad de esta clase reside en el método build. El método build crea un índice de búsqueda utilizando la biblioteca Whoosh. Toma una lista de colecciones como argumento y luego lee cada archivo en la lista y agrega sus contenidos al índice. El índice se crea en la ruta especificada en la variable self.index_path.\n","\n","Si la lista de colecciones es un directorio, la función recorre cada archivo en el directorio y agrega sus contenidos al índice. Si la lista de colecciones es un archivo de texto (.txt), la función lee cada línea del archivo de texto y utiliza la biblioteca BeautifulSoup para extraer el contenido HTML de cada URL y agregarlo al índice. Si la lista de colecciones es un archivo ZIP (.zip), la función lee cada archivo dentro del archivo ZIP y agrega sus contenidos al índice.\n","\n","Finalmente ncontramos el método commit el cual simplemente hace que los documentos agregados estén disponibles para la búsqueda. Esta función confirma los cambios y escribe los documentos agregados en el índice en el disco. \n","\n","\n","**WhooshIndex**\n","\n","La clase WhooshIndex se utiliza para realizar diferentes operaciones de búsqueda y análisis de texto sobre un índice Whoosh. Cada uno de los métodos es autodescriptivo por su nombre y simplemente sustituye operaciones básicas sobre un Indice Whoosh. \n","\n","\n","Decidimos eliminar las clases abstractas Index y Builder ya que no tenian uso al estar definidas inmediatamente despues. \n"]},{"cell_type":"markdown","metadata":{"id":"YEOpKvZi9cos"},"source":["## Ejercicio 1.2: Búsqueda (2pt)\n","\n","Implementar la clase WhooshSearcher como subclase de Searcher."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XrowpYaLmMxX"},"outputs":[],"source":["import math\n","import re\n","\n","def from_query_to_terms(text):\n","    return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"IGtH0CpF9owH"},"outputs":[],"source":["class WhooshSearcher():\n","    \"\"\"\n","    Whoosh Searcher.\n","    Class to search a Whoosh index.\n","    \n","    @param index_path : str\n","        Path to the index directory.\n","    @attribute index : whoosh.index.Index\n","        Whoosh index.\n","    @attribute searcher : whoosh.searching.Searcher\n","        Whoosh searcher.\n","    @attribute qparser : whoosh.qparser.QueryParser\n","        Whoosh query parser.\n","    \"\"\"\n","    index = None\n","    searcher = None\n","    qparser = None\n","\n","    def __init__(self, index_path):\n","        self.index = whoosh.index.open_dir(index_path)\n","        self.searcher = self.index.searcher()\n","        self.qparser = QueryParser(\"content\", schema=self.index.schema)\n","\n","    def search(self, query, cutoff):\n","        \"\"\"\n","        search(query, cutoff)\n","            - Search the index for a query.\n","        @param query: query to search\n","        @param cutoff: number of results to return\n","        @return: list of tuples (path, score)\n","        \"\"\"\n","        result = []\n","        search_results = list(self.searcher.search(self.qparser.parse(query)).items())[:cutoff]\n","\n","        for docid, score in search_results:\n","            result.append((self.index.reader().stored_fields(docid)['path'], score))\n","        return result\n","        \n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DNDHrfjN-zTV"},"source":["### Explicación/documentación\n","\n","La clase WhooshSearcher encapsula un buscador \"Searcher\" orientado a obtener resultados sobre el índice que obtiene como argumento.\n","Para cumplir su propósito hace uso de la librería whoosh, para abrir y leer el índice. Además del constructor, la clase WhooshSearcher\n","contiene un método search. Éste método devuelve los N documentos del índice que más puntuación obtienen, estando la puntuación directamente\n","relacionada con las apariciones y frecuencia de los términos introducidos como \"query\" en el argumento 1, siendo N el argumento 2.\n","Ya que utilizamos las funciones implementadas con la librería whoosh, no podemos ver de manera directa el código que se utiliza para obtener los\n","scores."]},{"cell_type":"markdown","metadata":{"id":"JH9u7bWi9pGc"},"source":["# Ejercicio 2: Modelo vectorial\n","\n","Implementar dos modelos de ránking propios, basados en el modelo vectorial."]},{"cell_type":"markdown","metadata":{"id":"SphKCsSN9udY"},"source":["## Ejercicio 2.1: Producto escalar (2.5pt)\n","\n","Implementar un modelo vectorial propio que utilice el producto escalar (sin dividir por las normas de los vectores) como función de ránking, por medio de la clase VSMDotProductSearcher, como subclase de Searcher.\n","\n","Este modelo hará uso de la clase Index y se podrá probar con la implementación WhooshIndex (puedes ver un ejemplo de esto en la celda de prueba).\n","\n","Además, la clase VSMDotProductSearcher será intercambiable con WhooshSearcher, como se puede ver en la celda de prueba, donde la función test_search utiliza una implementación u otra sin distinción."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6JQ2xbth-A-G"},"outputs":[],"source":["from whoosh.searching import Searcher\n","\n","\n","\n","def tf(freq):\n","    \"\"\"\n","    tf(freq)\n","        - Return the term frequency.\n","    @param freq: term frequency\n","    \"\"\"\n","    return 1 + math.log2(freq) if freq > 0 else 0\n","\n","\n","def idf(df, n):\n","    \"\"\"\n","    idf(df, n)\n","        - Return the inverse document frequency.\n","    @param df: document frequency\n","    @param n: number of documents\n","    \"\"\"\n","    return math.log2((n + 1) / (df + 0.5))\n","\n","\n","\n","class VSMDotProductSearcher():\n","    \"\"\"\n","    VSM Dot Product Searcher\n","    Class to search a Whoosh index using the VSM model with the dot product.\n","\n","    @param index_path : str\n","        - Path to the index directory.\n","    \"\"\"\n","    def __init__(self, index):\n","        self.index = index\n","\n","    def search(self, query, cutoff):\n","        \"\"\"\n","        search(query, cutoff)\n","            - Search the index for a query.\n","        @param query: query to search\n","        @param cutoff: number of results to return\n","        @return: list of tuples (path, score)\n","        \"\"\"\n","        docs=[]\n","        terms = from_query_to_terms(query)\n","        for doc_id in range(self.index.ndocs()):\n","            score=0\n","            for term in terms:\n","                score += tf(self.index.term_freq(term, doc_id)) * idf(self.index.doc_freq(term), self.index.ndocs())\n","            if score > 0:\n","                docs.append([self.index.doc_path(doc_id), score])\n","        docs.sort(key=lambda x: x[1], reverse=True)\n","        return docs[0:cutoff]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UgumTPoT-1WD"},"source":["### Explicación/documentación\n","\n","La clase VSMDotProductSearcher es una subclase de Searcher que implementa un modelo vectorial de búsqueda utilizando el producto escalar sin dividir por las normas de los vectores. Esto se logra mediante el cálculo de la puntuación de cada documento en función de la frecuencia del término en el documento y la frecuencia del término en el índice. Para cada documento en el índice, se calcula su puntuación sumando las puntuaciones de todos los términos en la consulta que aparecen en el documento. Si la puntuación del documento es mayor que cero, se agrega a la lista de resultados.\n","\n","La función tf calcula la frecuencia de término ajustada (tf) para un término en un documento. La función idf calcula el factor de ponderación inverso del documento (idf) para un término en el índice. La suma de los valores tf y idf para cada término se utiliza para calcular la puntuación de un documento.\n","\n","La función search toma una consulta y un límite superior de resultados, y devuelve una lista de los documentos más relevantes ordenados por puntuación decreciente. La consulta se divide en términos y se busca la puntuación de cada documento en función de los términos en la consulta. Los resultados se devuelven como una lista de pares de documentos y puntuaciones.\n","\n","Esta implementación es justificable porque el producto escalar es una medida común de similitud en espacios vectoriales, y no dividir por las normas de los vectores puede simplificar los cálculos y no afectar significativamente la calidad de los resultados. Además, la implementación utiliza las funciones tf e idf estándar en el procesamiento de lenguaje natural, lo que mejora la precisión de los resultados. En general, la implementación de VSMDotProductSearcher es una forma razonable y eficiente de realizar búsquedas en un índice de Whoosh."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8o5TvvE5-BVK"},"source":["### Ejercicio\n","\n","Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n","\n","### Solución\n","\n","Para obtener el score con producto escalar multiplicamos la frecuencia de los términos en el documento por la función idf, siendo esta \"log2((n + 1) / (df + 0.5))\".\n","Para obtener un score lo más grande posible, por lo tanto, hay dos relaciones que podemos modificar.\n","Por un lado podemos aumentar la frecuencia de cada término (respecto del documento), que modifica directamente el valor del score.\n","Por otro lado, necesitamos que la función idf devuelva el valor más alto posible. Como el número de documentos es constante, lo más optimo es que (df + 0.5) sea\n","lo menor posible, siendo df la frecuencia total de los términos en búsqueda.\n","Por lo tanto, para obtener el mejor resultado posible hay que conseguir un balance entre la frecuencia de los términos por documento y la frecuencia de los términos en relación al resto de documentos del índice. Como la función idf escala de forma logarítmica, y en cambio, la cantidad de apariciones de cada término escala de forma lineal, hemos decidido utilizar un documento con la query utilizada (\"obama family tree\") repetida un gran número de veces."]},{"cell_type":"markdown","metadata":{"id":"qPPNbWNe-HcR"},"source":["## Ejercicio 2.2: Coseno (2pt)\n","\n","Refinar la implementación del modelo para que calcule el coseno, definiendo para ello una clase VSMCosineSearcher. Para ello se necesitará extender Builder (o WhooshBuilder) con el cálculo de los módulos de los vectores, que deberán almacenarse en un fichero, en la carpeta de índice junto a los ficheros que genera cada índice. \n","\n","Pensad en qué parte del diseño interesa hacer esto, en concreto, qué clase y en qué momento tendría que calcular, devolver y/o almacenar estos módulos."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"9n30WQX_-RMR"},"outputs":[],"source":["class VSMCosineSearcher():\n","    \"\"\"\n","    VSMCosineSearcher\n","    Class to search a Whoosh index using the VSM with cosine similarity.\n","    \n","    @param index : WhooshSearcher\n","        - Whoosh index\n","    @attribute index : WhooshSearcher\n","        - Whoosh index\n","    @attribute modules : list\n","        - List of modules of the documents in the index\n","    \"\"\"\n","    index = None\n","    modules = []\n","\n","    def __init__(self, index):\n","        self.index = index\n","        self.modules = []\n","        for doc_id in range(index.ndocs()):\n","            div, divd, divq = 0, 0, 0\n","            for vec in index.doc_terms_with_freq(doc_id):\n","                divd = (tf(vec[1])) ** 2 \n","                divq = (idf(index.doc_freq(vec[0]), index.ndocs())) ** 2\n","                div += divd*divq\n","            div=math.sqrt(div)\n","            self.modules.append(div)\n","\n","\n","\n","\n","    def search(self, query, cutoff):\n","        \"\"\"\n","        search(query, cutoff)\n","            - Search the index for a query.\n","        @param query: query to search\n","        @param cutoff: number of results to return\n","        @return: list of tuples (path, score)\n","        \"\"\"\n","        docs=[]\n","        terms = from_query_to_terms(query)\n","        for doc_id in range(self.index.ndocs()):\n","            score=0\n","            for term in terms:\n","                score += ((tf(self.index.term_freq(term, doc_id)) * idf(self.index.doc_freq(term), self.index.ndocs())) / self.modules[doc_id])\n","            if score > 0:\n","                docs.append([self.index.doc_path(doc_id), score])\n","        docs.sort(key=lambda x: x[1], reverse=True)\n","        return docs[0:cutoff]\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UJqVKt6o-2oB"},"source":["### Explicación/documentación\n","\n","En esta nueva implementación, se ha refinado el modelo vectorial anterior para que calcule el coseno en lugar del producto escalar como función de ranking, lo que puede proporcionar resultados más precisos. Para ello, se ha creado la clase VSMCosineSearcher, que extiende la clase Searcher para permitir la búsqueda en el índice.\n","\n","La parte más importante de esta nueva implementación es el cálculo de los módulos de los vectores. Los módulos se calculan en el constructor de VSMCosineSearcher y se almacenan en una lista llamada modules. Para calcular el módulo de un vector, se utiliza la fórmula:\n","\n","\n","cos = sum(tf(w) * idf(w))/ sqrt(sum(tf(w)^2 * idf(w)^2))\n","donde 𝑡𝑓 mide la “importancia” de los términos en los documentos e 𝑖𝑑𝑓 mide el poder de discriminación del término\n","\n","Al inicializar el buscador guardamos la parte de abajo del algortimo, sqrt(sum(tf(w)^2 * idf(w)^2)). Esto nos ayuda a tener parte de la función ya realizada a la hora de satisfacer cada búsqueda.\n","\n","Cada vez que se utiliza el método \"search\" de la clase, ejecuta el resto de la fórmula, sumando el score de los términos dependientes de la query\n","\n","Finalmente, se ordenan los resultados por score en orden descendente y se devuelve un máximo de cutoff resultados.\n","\n","En cuanto al almacenamiento de los módulos, se ha decidido que se almacenen en un fichero en la carpeta del índice, junto con los ficheros que genera cada índice. Para ello, se debe extender la clase Builder o WhooshBuilder para que calcule y almacene los módulos en el momento en que se crea el índice."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wh83cdIu-Re1"},"source":["### Ejercicio\n","\n","Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n","\n","### Solución\n","\n","Para obtener el mismo resultado para la búsqueda de \"obama family tree\" con este nuevo CosineSearcher, la implementación que debemos seguir es similar a la utilizada para el modelo de producto escalar, ya que a pesar de que la fórmula que utilizamos para obtener el score varien, las dependencias se mantienen, estando relacionado directamente con la frecuencia de los elementos de la query por documento. Por ello, igualmente, hemos utilizado un nuevo documento llenandolo de una gran repetición de los términos de la búsqueda para obtener el mayor score posible."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3Wf1RFu8-V8P"},"source":["# Ejercicio 3: Estadísticas de frecuencias (1pt)\n","\n","Utilizando las funcionalidades de la clase Index, implementar una función term_stats que calcule a) las frecuencias totales en la colección de los términos, ordenadas de mayor a menor, y b) el número de documentos que contiene cada término, igualmente de mayor a menor. Visualizar las estadísticas obtenidas en dos gráficas en escala log-log (dos gráficas por cada colección, seis gráficas en total), que se mostrarán en el cuaderno entregado.\n","\n","De esta forma, podrás comprobar si las estadísticas de la colección siguen algún tipo de comportamiento esperado (como la conocida [Ley de Zipf](https://es.wikipedia.org/wiki/Ley_de_Zipf))."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def get_file_name(path):\n","    dir_name, file_name = os.path.split(path)\n","\n","    match = re.match(r'^(.*)\\.(.*?)$', file_name)\n","    if match:\n","        return match.group(1)\n","    else:\n","        return file_name\n","\n","def term_stats(index):\n","    # las frecuencias totales en la colección de los términos, ordenadas de mayor a menor,\n","    freqs = sorted(index.all_terms_with_freq(), key=lambda x: x[1], reverse=True)\n","    # Visualizar las estadísticas obtenidas en dos gráficas en escala log-log\n","    plt.loglog([x[1] for x in freqs])\n","    plt.xlabel(\"Term\")\n","    plt.ylabel(\"Frequency\")\n","    plt.title(\"Total term frequency on : \"+get_file_name(index.path))\n","    plt.grid(True)\n","    plt.savefig(\"term_frequency_\"+get_file_name(index.path)+\".png\")\n","    plt.clf()\n","    # el número de documentos que contiene cada término, igualmente de mayor a menor.\n","    docs_per_term = sorted([(term, index.doc_freq(term)) for term in index.all_terms()], key=lambda x: x[1], reverse=True)\n","    plt.loglog([x[1] for x in docs_per_term])\n","    plt.xlabel(\"Term\")\n","    plt.ylabel(\"Documents\")\n","    plt.title(\"Number of documents per term on : \"+get_file_name(index.path))\n","    plt.grid(True)\n","    plt.savefig(\"documents_per_term_\"+get_file_name(index.path)+\".png\")\n","    plt.clf()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SlXLFNH3-4MH"},"source":["### Explicación/documentación\n","Para obtener los diferentes plots, en el código al final del documento hemos añadido una linea justo después de la creación del Indice que llamaba a esta funcion. Esta función simplemente genera dos tipos de gráficos para cada una de las colecciones. Una gráfica muestra el numero de documentos en los que aparece cada término ordenado de mayor numero de apariciones a menor (documents_per_term_XXX.png) y la otra gráfica muestra  la frecuencia de cada uno de los términos enla colección. \n","\n","Es importante destacar que cuanto mayor numero de datos se recogen mas se parece la grafica resultante a la ley de Zipf la cual sigue la siguiente función P~1/nª.\n","\n","\n","### Imágenes de los plots:\n","![Alt text](plots/documents_per_term_toy.png)\n","![Alt text](plots/term_frequency_toy.png)\n","![Alt text](plots/documents_per_term_urls.png)\n","![Alt text](plots/term_frequency_urls.png)\n","![Alt text](plots/documents_per_term_docs.png)\n","![Alt text](plots/term_frequency_docs.png)\n"]},{"cell_type":"markdown","metadata":{"id":"UfgNDMM6-e7k"},"source":["# Celda de prueba\n","\n","Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta *collections* en el mismo directorio que este *notebook*. El fichero *toy.zip* hay que descomprimirlo para indexar la carpeta que contiene."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"VXS8648MzPO3","outputId":"f0523523-9462-499e-ba4a-b6be2679718a"},"outputs":[{"name":"stdout","output_type":"stream","text":["=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Done ( 0.026472091674804688 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 4\n","Vocabulary size: 39\n","  Top 5 most frequent terms:\n","\taa\t9.0=9.0\n","\tbb\t5.0=5.0\n","\tsleep\t5.0=5.0\n","\tcc\t3.0=3.0\n","\tdie\t2.0=2.0\n","\n","\n","  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n","  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n","  Docs containing the word'cc': 2\n","Done ( 0.0013790130615234375 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'aa dd'\n","\n","Done ( 0.0009815692901611328 seconds )\n","\n","  VSMDotProductSearcher for query 'aa dd'\n","4.0 \t ./collections/toy/d1.txt\n","1.7369655941662063 \t ./collections/toy/d2.txt\n","1.0 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0009028911590576172 seconds )\n","\n","  VSMCosineSearcher for query 'aa dd'\n","1.0 \t ./collections/toy/d2.txt\n","0.7427813527082074 \t ./collections/toy/d1.txt\n","0.5773502691896258 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0014188289642333984 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/urls.txt\n","Done ( 3.5340583324432373 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 3\n","Vocabulary size: 6058\n","  Top 5 most frequent terms:\n","\tentropy\t375.0=375.0\n","\tbias\t201.0=201.0\n","\tsystem\t154.0=154.0\n","\tdisplaystyle\t138.0=138.0\n","\theat\t111.0=111.0\n","\n","\n","  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n","  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n","  Docs containing the word'wikipedia': 3\n","Done ( 0.14136505126953125 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'information probability'\n","2.9256657209314 \t https://en.wikipedia.org/wiki/Entropy\n","2.762888436480296 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","2.0995062376814877 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.00875091552734375 seconds )\n","\n","  VSMDotProductSearcher for query 'information probability'\n","2.1879764911644646 \t https://en.wikipedia.org/wiki/Entropy\n","1.301295829346397 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","1.155870467654375 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.0006482601165771484 seconds )\n","\n","  VSMCosineSearcher for query 'information probability'\n","0.024351038531673412 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","0.01743041087762228 \t https://en.wikipedia.org/wiki/Entropy\n","0.00959731136934486 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.0007748603820800781 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/docs1k.zip\n","Done ( 103.94468092918396 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 1.807939052581787 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.2532188892364502 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 4.182596921920776 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 2.493206024169922 seconds )\n","\n","=================================================================\n","Testing indices and search on 3 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Collection: ./collections/urls.txt\n","Collection: ./collections/docs1k.zip\n","Done ( 119.31910181045532 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 1.9052791595458984 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.17962408065795898 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 2.6829638481140137 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 3.0209431648254395 seconds )\n","\n"]}],"source":["import os\n","import shutil\n","import time\n","\n","def clear (index_path: str):\n","    if os.path.exists(index_path): shutil.rmtree(index_path)\n","    else: print(\"Creating \" + index_path)\n","    os.makedirs(index_path)\n","\n","def test_collection(collection_paths: list, index_path: str, word: str, query: str):\n","    start_time = time.time()\n","    print(\"=================================================================\")\n","    print(\"Testing indices and search on \" + str(len(collection_paths)) + \" collections\")\n","\n","    # Let's create the folder if it did not exist\n","    # and delete the index if it did\n","    clear(index_path)\n","\n","    # We now test building an index\n","    test_build(WhooshBuilder(index_path), collection_paths)\n","\n","    # We now inspect the index\n","    index = WhooshIndex(index_path)\n","    test_read(index, word)\n","\n","    print(\"------------------------------\")\n","    print(\"Checking search results\")\n","    test_search(WhooshSearcher(index_path), query, 5)\n","    test_search(VSMDotProductSearcher(WhooshIndex(index_path)), query, 5)\n","    test_search(VSMCosineSearcher(WhooshIndex(index_path)), query, 5)\n","\n","def test_build(builder, collections: list):\n","    stamp = time.time()\n","    print(\"Building index with\", type(builder))\n","    for collection in collections:\n","        print(\"Collection:\", collection)\n","        # This function should index the received collection and add it to the index\n","        builder.build(collection)\n","    # When we commit, the information in the index becomes persistent\n","    # we can also save any extra information we may need\n","    # (and that cannot be computed until the entire collection is scanned/indexed)\n","    builder.commit()\n","    print(\"Done (\", time.time() - stamp, \"seconds )\")\n","    print()\n","\n","def test_read(index, word):\n","    stamp = time.time()\n","    print(\"Reading index with\", type(index))\n","    print(\"Collection size:\", index.ndocs())\n","    print(\"Vocabulary size:\", len(index.all_terms()))\n","    terms = index.all_terms_with_freq()\n","    terms.sort(key=lambda tup: tup[1], reverse=True)\n","    print(\"  Top 5 most frequent terms:\")\n","    for term in terms[0:5]:\n","        print(\"\\t\" + term[0] + \"\\t\" + str(term[1]) + \"=\" + str(index.total_freq(term)))\n","    print()\n","    # More tests\n","    doc_id = 0\n","    print()\n","    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n","    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n","    print(\"  Docs containing the word'\" + word + \"':\", index.doc_freq(word))\n","    print(\"Done (\", time.time() - stamp, \"seconds )\")\n","    print()\n","\n","\n","def test_search (engine, query, cutoff):\n","    stamp = time.time()\n","    print(\"  \" + engine.__class__.__name__ + \" for query '\" + query + \"'\")\n","    for path, score in engine.search(query, cutoff):\n","        print(score, \"\\t\", path)\n","    print()\n","    print(\"Done (\", time.time() - stamp, \"seconds )\")\n","    print()\n","\n","\n","index_root_dir = \"./index/\"\n","collections_root_dir = \"./collections/\"\n","test_collection ([collections_root_dir + \"toy/\"], index_root_dir + \"toy\", \"cc\", \"aa dd\")\n","test_collection ([collections_root_dir + \"urls.txt\"], index_root_dir + \"urls\", \"wikipedia\", \"information probability\")\n","test_collection ([collections_root_dir + \"docs1k.zip\"], index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n","test_collection ([collections_root_dir + \"toy/\", collections_root_dir + \"urls.txt\", collections_root_dir + \"docs1k.zip\"], index_root_dir + \"all_together\", \"seat\", \"obama family tree\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gTtUTF7j_QdF"},"source":["### Salida obtenida por el estudiante"]},{"cell_type":"raw","metadata":{},"source":["=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Done ( 0.02629995346069336 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 4\n","Vocabulary size: 39\n","  Top 5 most frequent terms:\n","\taa\t9.0=9.0\n","\tbb\t5.0=5.0\n","\tsleep\t5.0=5.0\n","\tcc\t3.0=3.0\n","\tdie\t2.0=2.0\n","\n","\n","  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n","  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n","  Docs containing the word'cc': 2\n","Done ( 0.003924846649169922 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'aa dd'\n","\n","Done ( 0.0007581710815429688 seconds )\n","\n","  VSMDotProductSearcher for query 'aa dd'\n","4.0 \t ./collections/toy/d1.txt\n","1.7369655941662063 \t ./collections/toy/d2.txt\n","1.0 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0008211135864257812 seconds )\n","\n","  VSMCosineSearcher for query 'aa dd'\n","1.0 \t ./collections/toy/d2.txt\n","0.7427813527082074 \t ./collections/toy/d1.txt\n","0.5773502691896258 \t ./collections/toy/d3.txt\n","\n","Done ( 0.0008189678192138672 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/urls.txt\n","Done ( 3.2534189224243164 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 3\n","Vocabulary size: 6058\n","  Top 5 most frequent terms:\n","\tentropy\t375.0=375.0\n","\tbias\t201.0=201.0\n","\tsystem\t154.0=154.0\n","\tdisplaystyle\t138.0=138.0\n","\theat\t111.0=111.0\n","\n","\n","  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox: 6\n","  Total frequency of word \"wikipedia\" in the collection: 27.0 occurrences over 3 documents\n","  Docs containing the word'wikipedia': 3\n","Done ( 0.18395709991455078 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'information probability'\n","2.9256657209314 \t https://en.wikipedia.org/wiki/Entropy\n","2.762888436480296 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","2.0995062376814877 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.007986068725585938 seconds )\n","\n","  VSMDotProductSearcher for query 'information probability'\n","2.1879764911644646 \t https://en.wikipedia.org/wiki/Entropy\n","1.301295829346397 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","1.155870467654375 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.001895904541015625 seconds )\n","\n","  VSMCosineSearcher for query 'information probability'\n","0.024351038531673412 \t https://en.wikipedia.org/wiki/Simpson's_paradox\n","0.01743041087762228 \t https://en.wikipedia.org/wiki/Entropy\n","0.00959731136934486 \t https://en.wikipedia.org/wiki/Bias\n","\n","Done ( 0.0006861686706542969 seconds )\n","\n","=================================================================\n","Testing indices and search on 1 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/docs1k.zip\n","Done ( 143.3548059463501 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 3.8520541191101074 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.5448360443115234 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 4.355900764465332 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 3.4219231605529785 seconds )\n","\n","=================================================================\n","Testing indices and search on 3 collections\n","Building index with <class '__main__.WhooshBuilder'>\n","Collection: ./collections/toy/\n","Collection: ./collections/urls.txt\n","Collection: ./collections/docs1k.zip\n","Done ( 128.99540185928345 seconds )\n","\n","Reading index with <class '__main__.WhooshIndex'>\n","Collection size: 998\n","Vocabulary size: 118052\n","  Top 5 most frequent terms:\n","\tfamily\t175767.0=175767.0\n","\ttree\t46705.0=46705.0\n","\thistory\t45744.0=45744.0\n","\tgenealogy\t45405.0=45405.0\n","\tsurname\t44965.0=44965.0\n","\n","\n","  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n","  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n","  Docs containing the word'seat': 119\n","Done ( 2.325594186782837 seconds )\n","\n","------------------------------\n","Checking search results\n","  WhooshSearcher for query 'obama family tree'\n","16.485501830411575 \t clueweb09-en0010-79-2218.html\n","15.887797097672806 \t clueweb09-en0010-57-32937.html\n","15.808957506888866 \t clueweb09-en0001-02-21241.html\n","15.611483818519556 \t clueweb09-en0008-45-29117.html\n","15.554096750861236 \t clueweb09-enwp01-59-16163.html\n","\n","Done ( 0.1670520305633545 seconds )\n","\n","  VSMDotProductSearcher for query 'obama family tree'\n","49.57650786955148 \t clueweb09-enwp01-59-16163.html\n","49.57650786955148 \t clueweb09-enwp02-06-15081.html\n","49.56357386531169 \t clueweb09-enwp03-00-6901.html\n","49.56357386531169 \t clueweb09-enwp03-07-2998.html\n","49.5444467358989 \t clueweb09-enwp00-00-9498.html\n","\n","Done ( 3.850494146347046 seconds )\n","\n","  VSMCosineSearcher for query 'obama family tree'\n","0.28421715940632897 \t clueweb09-en0010-79-2218.html\n","0.2263196301079154 \t clueweb09-en0009-30-2768.html\n","0.22480048420460652 \t clueweb09-en0001-02-21241.html\n","0.22386290177273055 \t clueweb09-en0009-30-2441.html\n","0.22349064069479563 \t clueweb09-en0009-30-2755.html\n","\n","Done ( 5.627086162567139 seconds )\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
